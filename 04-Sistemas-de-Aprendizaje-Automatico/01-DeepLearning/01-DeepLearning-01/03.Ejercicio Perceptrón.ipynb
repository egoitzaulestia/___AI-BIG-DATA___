{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59c91e6-31b4-4dbc-94cc-f744eccdd5d1",
   "metadata": {},
   "source": [
    "# **Ejercicio Perceptrón**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb64230-5217-499e-94e5-a3ae5407ce2b",
   "metadata": {},
   "source": [
    "Partiendo del ejemplo anterior:\n",
    "1. Cambia el número de observaciones a 100.000 y comprueba qué sucede\n",
    "2. Cambia el número de observaciones a 1.000.000 y comprueba qué sucede\n",
    "3. Juega con el ratio de aprendizaje con los siguientes valores: 0.0001, 0.001, 0.1, 1\n",
    "4. Prueba cambiar la función de pérdida.  En el ejercicio, minimizamos el error cuadrático medio.  Esta vez minimiza el error absoluto, que es básicamente el sumatorio de los deltas en valor absoluto $$ \\Sigma_i = |y_i-t_i| $$\n",
    "5. Crea una nueva función de activación que sea f(x,z) = 13 * xs + 7 * zs -12 y comprueba si el algoritmo funciona de la misma manera.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa16e4",
   "metadata": {},
   "source": [
    "## **1. Cambia el número de observaciones a 100.000 y comprueba qué sucede**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7eda35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 10:31:46.466435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos librerías\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc9b5a",
   "metadata": {},
   "source": [
    "#### **Generamos los datos (100_000)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ba5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 100_000\n",
    "\n",
    "np.random.seed(123)\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# Hasta aquí hemos generado los mismos datos, ahora los guardamos\n",
    "np.savez('data/data_100_000', inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaaacbf",
   "metadata": {},
   "source": [
    "#### **Resloviendo con Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1087f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el conjunto de datos\n",
    "training_data = np.load('data/data_100_000.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff5b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 610us/step - loss: 2.6568\n",
      "Epoch 2/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 692us/step - loss: 0.3414\n",
      "Epoch 3/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759us/step - loss: 0.3440\n",
      "Epoch 4/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558us/step - loss: 0.3433\n",
      "Epoch 5/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 559us/step - loss: 0.3423\n",
      "Epoch 6/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 642us/step - loss: 0.3440\n",
      "Epoch 7/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 798us/step - loss: 0.3441\n",
      "Epoch 8/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 568us/step - loss: 0.3432\n",
      "Epoch 9/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 660us/step - loss: 0.3411\n",
      "Epoch 10/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 695us/step - loss: 0.3435\n",
      "Epoch 11/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 950us/step - loss: 0.3462\n",
      "Epoch 12/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 751us/step - loss: 0.3435\n",
      "Epoch 13/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 760us/step - loss: 0.3438\n",
      "Epoch 14/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 658us/step - loss: 0.3427\n",
      "Epoch 15/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 626us/step - loss: 0.3426\n",
      "Epoch 16/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 601us/step - loss: 0.3433\n",
      "Epoch 17/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 608us/step - loss: 0.3428\n",
      "Epoch 18/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 796us/step - loss: 0.3430\n",
      "Epoch 19/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step - loss: 0.3419\n",
      "Epoch 20/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 531us/step - loss: 0.3430\n",
      "Epoch 21/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 616us/step - loss: 0.3446\n",
      "Epoch 22/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 716us/step - loss: 0.3437\n",
      "Epoch 23/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 703us/step - loss: 0.3421\n",
      "Epoch 24/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 682us/step - loss: 0.3425\n",
      "Epoch 25/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 693us/step - loss: 0.3430\n",
      "Epoch 26/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 780us/step - loss: 0.3418\n",
      "Epoch 27/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 929us/step - loss: 0.3421\n",
      "Epoch 28/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 823us/step - loss: 0.3435\n",
      "Epoch 29/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 691us/step - loss: 0.3440\n",
      "Epoch 30/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 615us/step - loss: 0.3431\n",
      "Epoch 31/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 606us/step - loss: 0.3447\n",
      "Epoch 32/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 851us/step - loss: 0.3414\n",
      "Epoch 33/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 712us/step - loss: 0.3439\n",
      "Epoch 34/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 698us/step - loss: 0.3415\n",
      "Epoch 35/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 651us/step - loss: 0.3421\n",
      "Epoch 36/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 908us/step - loss: 0.3422\n",
      "Epoch 37/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 843us/step - loss: 0.3440\n",
      "Epoch 38/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 938us/step - loss: 0.3435\n",
      "Epoch 39/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 749us/step - loss: 0.3427\n",
      "Epoch 40/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 799us/step - loss: 0.3417\n",
      "Epoch 41/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 955us/step - loss: 0.3445\n",
      "Epoch 42/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 809us/step - loss: 0.3439\n",
      "Epoch 43/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 756us/step - loss: 0.3431\n",
      "Epoch 44/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 653us/step - loss: 0.3427\n",
      "Epoch 45/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 799us/step - loss: 0.3435\n",
      "Epoch 46/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 542us/step - loss: 0.3417\n",
      "Epoch 47/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step - loss: 0.3431\n",
      "Epoch 48/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 659us/step - loss: 0.3409\n",
      "Epoch 49/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 683us/step - loss: 0.3434\n",
      "Epoch 50/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 673us/step - loss: 0.3419\n",
      "Epoch 51/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 542us/step - loss: 0.3425\n",
      "Epoch 52/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 791us/step - loss: 0.3434\n",
      "Epoch 53/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 788us/step - loss: 0.3435\n",
      "Epoch 54/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3440\n",
      "Epoch 55/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 619us/step - loss: 0.3435\n",
      "Epoch 56/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 588us/step - loss: 0.3436\n",
      "Epoch 57/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 593us/step - loss: 0.3422\n",
      "Epoch 58/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 645us/step - loss: 0.3424\n",
      "Epoch 59/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 684us/step - loss: 0.3422\n",
      "Epoch 60/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 747us/step - loss: 0.3422\n",
      "Epoch 61/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 849us/step - loss: 0.3435\n",
      "Epoch 62/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 636us/step - loss: 0.3422\n",
      "Epoch 63/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 680us/step - loss: 0.3447\n",
      "Epoch 64/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 742us/step - loss: 0.3440\n",
      "Epoch 65/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 719us/step - loss: 0.3434\n",
      "Epoch 66/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 740us/step - loss: 0.3434\n",
      "Epoch 67/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 715us/step - loss: 0.3418\n",
      "Epoch 68/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 561us/step - loss: 0.3416\n",
      "Epoch 69/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 583us/step - loss: 0.3443\n",
      "Epoch 70/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 523us/step - loss: 0.3435\n",
      "Epoch 71/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 695us/step - loss: 0.3442\n",
      "Epoch 72/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 495us/step - loss: 0.3418\n",
      "Epoch 73/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483us/step - loss: 0.3438\n",
      "Epoch 74/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 624us/step - loss: 0.3443\n",
      "Epoch 75/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 609us/step - loss: 0.3441\n",
      "Epoch 76/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 607us/step - loss: 0.3419\n",
      "Epoch 77/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 577us/step - loss: 0.3437\n",
      "Epoch 78/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 477us/step - loss: 0.3435\n",
      "Epoch 79/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3440\n",
      "Epoch 80/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 582us/step - loss: 0.3415\n",
      "Epoch 81/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 514us/step - loss: 0.3439\n",
      "Epoch 82/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 763us/step - loss: 0.3431\n",
      "Epoch 83/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 537us/step - loss: 0.3437\n",
      "Epoch 84/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 539us/step - loss: 0.3435\n",
      "Epoch 85/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 543us/step - loss: 0.3438\n",
      "Epoch 86/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 679us/step - loss: 0.3432\n",
      "Epoch 87/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3446\n",
      "Epoch 88/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 653us/step - loss: 0.3429\n",
      "Epoch 89/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 647us/step - loss: 0.3413\n",
      "Epoch 90/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 754us/step - loss: 0.3436\n",
      "Epoch 91/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 869us/step - loss: 0.3423\n",
      "Epoch 92/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 717us/step - loss: 0.3429\n",
      "Epoch 93/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 704us/step - loss: 0.3424\n",
      "Epoch 94/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 676us/step - loss: 0.3425\n",
      "Epoch 95/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 528us/step - loss: 0.3449\n",
      "Epoch 96/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 549us/step - loss: 0.3424\n",
      "Epoch 97/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 541us/step - loss: 0.3438\n",
      "Epoch 98/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 546us/step - loss: 0.3419\n",
      "Epoch 99/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 633us/step - loss: 0.3421\n",
      "Epoch 100/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510us/step - loss: 0.3430\n",
      "Epoch 101/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483us/step - loss: 0.3430\n",
      "Epoch 102/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 551us/step - loss: 0.3446\n",
      "Epoch 103/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510us/step - loss: 0.3434\n",
      "Epoch 104/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 547us/step - loss: 0.3446\n",
      "Epoch 105/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502us/step - loss: 0.3437\n",
      "Epoch 106/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 545us/step - loss: 0.3440\n",
      "Epoch 107/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 484us/step - loss: 0.3429\n",
      "Epoch 108/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 490us/step - loss: 0.3430\n",
      "Epoch 109/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3420\n",
      "Epoch 110/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 493us/step - loss: 0.3452\n",
      "Epoch 111/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 531us/step - loss: 0.3428\n",
      "Epoch 112/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 553us/step - loss: 0.3414\n",
      "Epoch 113/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 538us/step - loss: 0.3430\n",
      "Epoch 114/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3415\n",
      "Epoch 115/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 490us/step - loss: 0.3439\n",
      "Epoch 116/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 492us/step - loss: 0.3412\n",
      "Epoch 117/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 533us/step - loss: 0.3428\n",
      "Epoch 118/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 611us/step - loss: 0.3433\n",
      "Epoch 119/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 689us/step - loss: 0.3438\n",
      "Epoch 120/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 493us/step - loss: 0.3437\n",
      "Epoch 121/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 555us/step - loss: 0.3426\n",
      "Epoch 122/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 740us/step - loss: 0.3435\n",
      "Epoch 123/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 588us/step - loss: 0.3423\n",
      "Epoch 124/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 551us/step - loss: 0.3438\n",
      "Epoch 125/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 566us/step - loss: 0.3436\n",
      "Epoch 126/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 491us/step - loss: 0.3432\n",
      "Epoch 127/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 585us/step - loss: 0.3436\n",
      "Epoch 128/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502us/step - loss: 0.3442\n",
      "Epoch 129/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 622us/step - loss: 0.3424\n",
      "Epoch 130/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 706us/step - loss: 0.3443\n",
      "Epoch 131/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 954us/step - loss: 0.3440\n",
      "Epoch 132/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 677us/step - loss: 0.3437\n",
      "Epoch 133/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 756us/step - loss: 0.3442\n",
      "Epoch 134/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 714us/step - loss: 0.3431\n",
      "Epoch 135/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 720us/step - loss: 0.3439\n",
      "Epoch 136/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558us/step - loss: 0.3442\n",
      "Epoch 137/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 626us/step - loss: 0.3413\n",
      "Epoch 138/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 595us/step - loss: 0.3433\n",
      "Epoch 139/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 589us/step - loss: 0.3428\n",
      "Epoch 140/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 553us/step - loss: 0.3426\n",
      "Epoch 141/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 531us/step - loss: 0.3423\n",
      "Epoch 142/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 896us/step - loss: 0.3431\n",
      "Epoch 143/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 597us/step - loss: 0.3437\n",
      "Epoch 144/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 646us/step - loss: 0.3416\n",
      "Epoch 145/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488us/step - loss: 0.3419\n",
      "Epoch 146/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 509us/step - loss: 0.3428\n",
      "Epoch 147/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 605us/step - loss: 0.3427\n",
      "Epoch 148/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 507us/step - loss: 0.3420\n",
      "Epoch 149/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 517us/step - loss: 0.3451\n",
      "Epoch 150/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 605us/step - loss: 0.3442\n",
      "Epoch 151/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 512us/step - loss: 0.3436\n",
      "Epoch 152/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 747us/step - loss: 0.3423\n",
      "Epoch 153/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.3457\n",
      "Epoch 154/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 744us/step - loss: 0.3420\n",
      "Epoch 155/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 687us/step - loss: 0.3444\n",
      "Epoch 156/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.3427\n",
      "Epoch 157/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 646us/step - loss: 0.3423\n",
      "Epoch 158/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 544us/step - loss: 0.3436\n",
      "Epoch 159/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 674us/step - loss: 0.3425\n",
      "Epoch 160/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 613us/step - loss: 0.3432\n",
      "Epoch 161/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 591us/step - loss: 0.3449\n",
      "Epoch 162/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 536us/step - loss: 0.3438\n",
      "Epoch 163/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 575us/step - loss: 0.3442\n",
      "Epoch 164/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 514us/step - loss: 0.3428\n",
      "Epoch 165/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496us/step - loss: 0.3424\n",
      "Epoch 166/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3427\n",
      "Epoch 167/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 481us/step - loss: 0.3438\n",
      "Epoch 168/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 495us/step - loss: 0.3427\n",
      "Epoch 169/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 530us/step - loss: 0.3423\n",
      "Epoch 170/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 520us/step - loss: 0.3437\n",
      "Epoch 171/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 491us/step - loss: 0.3429\n",
      "Epoch 172/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 723us/step - loss: 0.3431\n",
      "Epoch 173/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 649us/step - loss: 0.3417\n",
      "Epoch 174/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 493us/step - loss: 0.3416\n",
      "Epoch 175/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488us/step - loss: 0.3450\n",
      "Epoch 176/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 494us/step - loss: 0.3436\n",
      "Epoch 177/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 522us/step - loss: 0.3438\n",
      "Epoch 178/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 547us/step - loss: 0.3428\n",
      "Epoch 179/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488us/step - loss: 0.3429\n",
      "Epoch 180/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 490us/step - loss: 0.3415\n",
      "Epoch 181/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488us/step - loss: 0.3434\n",
      "Epoch 182/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 515us/step - loss: 0.3414\n",
      "Epoch 183/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 492us/step - loss: 0.3427\n",
      "Epoch 184/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 481us/step - loss: 0.3421\n",
      "Epoch 185/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 507us/step - loss: 0.3428\n",
      "Epoch 186/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 484us/step - loss: 0.3439\n",
      "Epoch 187/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504us/step - loss: 0.3411\n",
      "Epoch 188/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516us/step - loss: 0.3429\n",
      "Epoch 189/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 493us/step - loss: 0.3431\n",
      "Epoch 190/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 519us/step - loss: 0.3428\n",
      "Epoch 191/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506us/step - loss: 0.3424\n",
      "Epoch 192/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 521us/step - loss: 0.3425\n",
      "Epoch 193/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 545us/step - loss: 0.3429\n",
      "Epoch 194/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 481us/step - loss: 0.3421\n",
      "Epoch 195/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 487us/step - loss: 0.3446\n",
      "Epoch 196/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483us/step - loss: 0.3442\n",
      "Epoch 197/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 593us/step - loss: 0.3425\n",
      "Epoch 198/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 491us/step - loss: 0.3434\n",
      "Epoch 199/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503us/step - loss: 0.3434\n",
      "Epoch 200/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 561us/step - loss: 0.3446\n",
      "Epoch 201/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 602us/step - loss: 0.3441\n",
      "Epoch 202/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 498us/step - loss: 0.3434\n",
      "Epoch 203/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 642us/step - loss: 0.3447\n",
      "Epoch 204/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510us/step - loss: 0.3429\n",
      "Epoch 205/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656us/step - loss: 0.3414\n",
      "Epoch 206/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 730us/step - loss: 0.3427\n",
      "Epoch 207/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.3445\n",
      "Epoch 208/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 688us/step - loss: 0.3415\n",
      "Epoch 209/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 778us/step - loss: 0.3443\n",
      "Epoch 210/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 941us/step - loss: 0.3448\n",
      "Epoch 211/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.3426\n",
      "Epoch 212/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 826us/step - loss: 0.3452\n",
      "Epoch 213/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 668us/step - loss: 0.3439\n",
      "Epoch 214/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 611us/step - loss: 0.3430\n",
      "Epoch 215/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 552us/step - loss: 0.3429\n",
      "Epoch 216/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 874us/step - loss: 0.3424\n",
      "Epoch 217/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 597us/step - loss: 0.3441\n",
      "Epoch 218/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 694us/step - loss: 0.3417\n",
      "Epoch 219/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 632us/step - loss: 0.3441\n",
      "Epoch 220/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3436\n",
      "Epoch 221/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 503us/step - loss: 0.3431\n",
      "Epoch 222/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 515us/step - loss: 0.3457\n",
      "Epoch 223/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 521us/step - loss: 0.3434\n",
      "Epoch 224/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 533us/step - loss: 0.3439\n",
      "Epoch 225/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 624us/step - loss: 0.3412\n",
      "Epoch 226/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 522us/step - loss: 0.3419\n",
      "Epoch 227/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 514us/step - loss: 0.3433\n",
      "Epoch 228/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 528us/step - loss: 0.3424\n",
      "Epoch 229/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 560us/step - loss: 0.3417\n",
      "Epoch 230/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 536us/step - loss: 0.3433\n",
      "Epoch 231/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 555us/step - loss: 0.3430\n",
      "Epoch 232/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502us/step - loss: 0.3431\n",
      "Epoch 233/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 491us/step - loss: 0.3433\n",
      "Epoch 234/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 565us/step - loss: 0.3429\n",
      "Epoch 235/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496us/step - loss: 0.3418\n",
      "Epoch 236/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506us/step - loss: 0.3418\n",
      "Epoch 237/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 573us/step - loss: 0.3426\n",
      "Epoch 238/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 583us/step - loss: 0.3421\n",
      "Epoch 239/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 482us/step - loss: 0.3426\n",
      "Epoch 240/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 486us/step - loss: 0.3425\n",
      "Epoch 241/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506us/step - loss: 0.3430\n",
      "Epoch 242/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 494us/step - loss: 0.3413\n",
      "Epoch 243/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513us/step - loss: 0.3431\n",
      "Epoch 244/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 484us/step - loss: 0.3445\n",
      "Epoch 245/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 505us/step - loss: 0.3417\n",
      "Epoch 246/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 566us/step - loss: 0.3431\n",
      "Epoch 247/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3424\n",
      "Epoch 248/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 488us/step - loss: 0.3438\n",
      "Epoch 249/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483us/step - loss: 0.3417\n",
      "Epoch 250/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 507us/step - loss: 0.3436\n",
      "Epoch 251/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3441\n",
      "Epoch 252/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 509us/step - loss: 0.3412\n",
      "Epoch 253/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 510us/step - loss: 0.3428\n",
      "Epoch 254/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 480us/step - loss: 0.3421\n",
      "Epoch 255/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 518us/step - loss: 0.3448\n",
      "Epoch 256/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 492us/step - loss: 0.3439\n",
      "Epoch 257/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 477us/step - loss: 0.3428\n",
      "Epoch 258/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 550us/step - loss: 0.3427\n",
      "Epoch 259/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 513us/step - loss: 0.3447\n",
      "Epoch 260/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 501us/step - loss: 0.3425\n",
      "Epoch 261/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485us/step - loss: 0.3420\n",
      "Epoch 262/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 483us/step - loss: 0.3421\n",
      "Epoch 263/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 506us/step - loss: 0.3439\n",
      "Epoch 264/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 480us/step - loss: 0.3440\n",
      "Epoch 265/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 542us/step - loss: 0.3422\n",
      "Epoch 266/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516us/step - loss: 0.3421\n",
      "Epoch 267/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.3457\n",
      "Epoch 268/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 975us/step - loss: 0.3434\n",
      "Epoch 269/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 864us/step - loss: 0.3416\n",
      "Epoch 270/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 893us/step - loss: 0.3424\n",
      "Epoch 271/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 644us/step - loss: 0.3434\n",
      "Epoch 272/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 670us/step - loss: 0.3433\n",
      "Epoch 273/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 816us/step - loss: 0.3420\n",
      "Epoch 274/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 674us/step - loss: 0.3449\n",
      "Epoch 275/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 737us/step - loss: 0.3446\n",
      "Epoch 276/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 586us/step - loss: 0.3437\n",
      "Epoch 277/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 562us/step - loss: 0.3430\n",
      "Epoch 278/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 605us/step - loss: 0.3431\n",
      "Epoch 279/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 570us/step - loss: 0.3443\n",
      "Epoch 280/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 647us/step - loss: 0.3415\n",
      "Epoch 281/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 597us/step - loss: 0.3432\n",
      "Epoch 282/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 559us/step - loss: 0.3447\n",
      "Epoch 283/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 533us/step - loss: 0.3424\n",
      "Epoch 284/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 601us/step - loss: 0.3418\n",
      "Epoch 285/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558us/step - loss: 0.3431\n",
      "Epoch 286/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 641us/step - loss: 0.3433\n",
      "Epoch 287/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 598us/step - loss: 0.3444\n",
      "Epoch 288/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 583us/step - loss: 0.3433\n",
      "Epoch 289/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 529us/step - loss: 0.3436\n",
      "Epoch 290/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 529us/step - loss: 0.3418\n",
      "Epoch 291/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 637us/step - loss: 0.3434\n",
      "Epoch 292/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 557us/step - loss: 0.3422\n",
      "Epoch 293/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 554us/step - loss: 0.3423\n",
      "Epoch 294/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 578us/step - loss: 0.3443\n",
      "Epoch 295/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 555us/step - loss: 0.3431\n",
      "Epoch 296/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 559us/step - loss: 0.3420\n",
      "Epoch 297/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step - loss: 0.3442\n",
      "Epoch 298/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 618us/step - loss: 0.3435\n",
      "Epoch 299/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 572us/step - loss: 0.3417\n",
      "Epoch 300/300\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 607us/step - loss: 0.3425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9821b57430>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declaramos una variable donde guardaremos el total de variables del modelo\n",
    "# En nuestro caso 2 xs y zs\n",
    "#input_size = 2\n",
    "\n",
    "# Declaramos el total de variables de salida del modelo\n",
    "# En nuestro caso 1\n",
    "output_size = 1\n",
    "\n",
    "# Definimos nuestro modelo como secuencial.\n",
    "# En esta etapa no hay datos involucrados, sólo definimos lo que queremos hacer.\n",
    "model = tf.keras.Sequential([\n",
    "                            # Cada capa se lista aquí\n",
    "                            # Definimos las características de la red\n",
    "                            # Dense, nos permite definir la operación matemática a ejecutar xw + b\n",
    "                            # Básicamente aplica la operación: output = activation(dot(input, kernel) + bias\n",
    "                            tf.keras.layers.Dense(output_size,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])\n",
    "\n",
    "# Definimos el ratio de aprendizaje\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Definimos la función de pérdida a minimizar.  En este caso el error cuadrático medio\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Definimos el modelo con los datos de entrada y targets.\n",
    "# Ejecutamos 100 ciclos\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fd960",
   "metadata": {},
   "source": [
    "#### **Extraemos los pesos y bias**\n",
    "\n",
    "La extracción de pesos y bias del modelo no es necesaria en el proceso de machine learning.  De hecho, no se ejecuta habitualmente, pero en este ejemplo simple nos sirve para comprobar que los resultados son correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e203381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.0133858],\n",
       "        [-2.9824955]], dtype=float32),\n",
       " array([5.0117683], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La extracción de pesos y biases es muy sencilla\n",
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e307f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0133858]\n",
      " [-2.9824955]]\n",
      "[5.0117683]\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "bias = model.layers[0].get_weights()[1]\n",
    "print(weights)\n",
    "print(bias)\n",
    "\n",
    "# éste es un modelo simple, pero en otros más complicados podemos tener cientos o miles de valores en estas variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef8ee6",
   "metadata": {},
   "source": [
    "#### **Generar las predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e56dde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.3],\n",
       "       [ 8.7],\n",
       "       [20.5],\n",
       "       ...,\n",
       "       [42.9],\n",
       "       [10.9],\n",
       "       [26.9]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos predichos\n",
    "model.predict_on_batch(training_data['inputs']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b3bfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29.5],\n",
       "       [ 8.3],\n",
       "       [21.7],\n",
       "       ...,\n",
       "       [42.8],\n",
       "       [10.7],\n",
       "       [26.6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos reales\n",
    "training_data['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce6f5c",
   "metadata": {},
   "source": [
    "#### **Pintamos los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea2b39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([30.261667,  8.703772, 20.51409 , ..., 42.868195, 10.881088,\n",
       "        26.900196], dtype=float32),\n",
       " array([29.46246055,  8.26681469, 21.69836826, ..., 42.77347206,\n",
       "        10.72972402, 26.6244577 ]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "472c9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGzCAYAAAAv9B03AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0OElEQVR4nO3deXxU5d338e8EyCTEJGySEMMSIAqKLIJQwLJY2bValxvF+giPUBEQES0aXEDFBHGpFoQabx7UVoW6tlXRRFEElX0tVlQWE4EYBUwCQhKS6/nD27mNYcmcObOcOZ/36zWvV3POuYZfTiH5+ruuOZfHGGMEAADgAjHhLgAAACBUCD4AAMA1CD4AAMA1CD4AAMA1CD4AAMA1CD4AAMA1CD4AAMA1CD4AAMA1CD4AAMA1CD4AAMA16oe7AH/s2bNHd9xxh5YuXaojR47ozDPP1MKFC9W9e3dJkjFG9913n3Jzc3Xw4EH16tVLTz75pM4555w6/xnV1dXau3evEhMT5fF4gvWtAAAAGxljVFZWprS0NMXEnLiv45jgc/DgQfXt21cDBw7U0qVL1bx5c+3YsUONGjXyXTNnzhw99thjeuaZZ3TmmWdq1qxZGjRokLZv367ExMQ6/Tl79+5Vy5Ytg/RdAACAYCosLFR6evoJz3ucsknpnXfeqY8++kgrVqw47nljjNLS0jRlyhTdcccdkqTy8nKlpKTooYce0o033linP6ekpESNGjVSYWGhkpKSbKsfAAAET2lpqVq2bKnvv/9eycnJJ7zOMcHn7LPP1pAhQ/T1119r+fLlOuOMMzRhwgSNGzdOkrRz5061a9dOGzZsULdu3XzjLr30UjVq1EjPPvvscd+3vLxc5eXlvq9/unElJSUEHwAAHKK0tFTJycmn/P3tmMXNO3fu1IIFC5SZmal33nlH48eP1+TJk/Xcc89JkoqKiiRJKSkpNcalpKT4zh1PTk6OkpOTfS+muQAAiF6OCT7V1dU677zzlJ2drW7duunGG2/UuHHjtGDBghrX/XJBsjHmpIuUs7KyVFJS4nsVFhYGpX4AABB+jgk+LVq00Nlnn13jWMeOHVVQUCBJSk1NlaRa3Z3i4uJaXaCf83q9SkpKqvECAADRyTHBp2/fvtq+fXuNY59//rlat24tScrIyFBqaqry8/N95ysqKrR8+XL16dMnpLUCAIDI5JiPs996663q06ePsrOz9V//9V9as2aNcnNzlZubK+nHKa4pU6YoOztbmZmZyszMVHZ2tho2bKhRo0aFuXoAABAJHBN8zj//fL322mvKysrS/fffr4yMDD3++OO69tprfddMmzZNR44c0YQJE3wPMMzLy6vzM3wAAEB0c8zH2UOlrh+HAwAAkSPqPs4OAAAQKIIPAABwDYIPAABwDYIPAABwDYIPAABwDYIPAAAIiepqo6OVVWGtgeADAACC7q+f7Fbb6W+pz+xlYQ0/jnmAIQAAcJ4tX3+v3877yPf1gcMVOsne4UFH8AEAALYrPVqpzjPzah1fecdAeevXC0NFPyL4AAAAW01/bateWF1w3HPpjRuGuJqaCD4AAMA2be5884Tnmp3mDWElx8fiZgAAELBVO/efNPRI0szfnh2iak6Mjg8AAAjIqQLPTwoPHAlyJadGxwcAAFhijKlz6JGk+R98qepqE8SKTo3gAwAA/LZ06z5lZL1V5+tj68VowbXdFRMTxs+yi6kuAADgJ3+6PJKUEFtPH/xxoE5PDP/iZoIPAACokwOHK3TeA/l+jbl7REf9374ZYe/0/ITgAwAATsnfLo8kvTn5Ap2TlhyEaqwj+AAAgBMyxvi1lucnn94/RA1jIy9msLgZAAAc18vrv7YUenZmD4/I0CPR8QEAAMdhZWpL+jH0RMp6nuOh4wMAAHx+qDhmOfTsnj0iokOPRMcHAAD8D6uB54VxvdSnXTObqwkOgg8AAAioy+MkTHUBAOBi3x0qd03okej4AADgWlYDz/u3D1BGswSbqwkNgg8AAC5j9dk8kjO7PD/HVBcAAC6yeE2Ba0OPRMcHAADXsDq19cHtA9TGoVNbv0TwAQAgylVVG7Wb7t4uz88RfAAAiGJd7stTyZFKS2OjLfRIBB8AAKKW1amt7bOGylu/ns3VRAYWNwMAEGVKjlQG9GyeaA09Eh0fAACiitXAI0Xn1NYvEXwAAIgSVkPPZw8MVVyD6O3y/BxTXQAAONzmwu8DmtpyS+iR6PgAAOBoVgPPjf3bKmtYR5uriXwEHwAAHCiQbSd25QyXx+OxuSJnIPgAAOAw4/+6Xm9vK7I01g0LmE+G4AMAgINYndp66rruGnJOqs3VOA/BBwAABzhWVa32dy21NNbtXZ6fI/gAABDheDaPfQg+AABEMKuh550p/XRWaqLN1TgfwQcAgAh0tLJKHe5529JYujwnRvABACDCMLUVPAQfAAAiRCDP5ll390VqdprX5oqiD8EHAIAI8Pa/92n83zZYGkuXp+4IPgAAhBlTW6FD8AEAIEwqq6qVafHZPNtnDZW3vns2F7ULwQcAgDCgyxMeMeEuAAAAt7Eaeq4+vyWhJ0B0fAAACJFvy8p1/oPvWhrr5h3V7UTwAQAgBJjaigwEHwAAgsxq6Hnkqi66snu6zdW4G8EHAIAg2fntIV346HJLY+nyBAfBBwCAIGBqKzI59lNdOTk58ng8mjJliu+YMUYzZ85UWlqa4uPjNWDAAG3bti18RQIAXKeq2lgOPc+MOZ/QE2SODD5r165Vbm6uOnfuXOP4nDlz9Nhjj2nevHlau3atUlNTNWjQIJWVlYWpUgCAm/z+v1er3XRre23tnj1CA85qbnNF+CXHBZ9Dhw7p2muv1dNPP63GjRv7jhtj9Pjjj+uuu+7S5Zdfrk6dOunZZ5/VDz/8oBdeeOGE71deXq7S0tIaLwAA/NXmzje18svvLI2lyxM6jgs+EydO1IgRI3TRRRfVOL5r1y4VFRVp8ODBvmNer1f9+/fXxx9/fML3y8nJUXJysu/VsmXLoNUOAIg+O749ZHlqa/kfBxB6QsxRi5sXL16sDRs2aO3atbXOFRUVSZJSUlJqHE9JSdFXX311wvfMysrS1KlTfV+XlpYSfgAAdcICZudxTPApLCzULbfcory8PMXFxZ3wul8+1dIYc9InXXq9Xnm9XtvqBABEP2OMMrKsreWRCD3h5Jjgs379ehUXF6t79+6+Y1VVVfrwww81b948bd++XdKPnZ8WLVr4rikuLq7VBQIAwKp/bNqjWxZvsjT2sweGKq4BO6qHk2OCz29+8xtt3bq1xrExY8aoQ4cOuuOOO9S2bVulpqYqPz9f3bp1kyRVVFRo+fLleuihh8JRMgAgyjC15XyOCT6JiYnq1KlTjWMJCQlq2rSp7/iUKVOUnZ2tzMxMZWZmKjs7Ww0bNtSoUaPCUTIAIEpUVxu1tfgxdYnQE0kcE3zqYtq0aTpy5IgmTJiggwcPqlevXsrLy1NiYmK4SwMAOFSPWe/qu0Pllsayo3rk8RhjTLiLiCSlpaVKTk5WSUmJkpKSwl0OACCMmNpyjrr+/nbcc3wAAAi2vd8fsRx6hp+bSuiJYFE11QUAQKAC6fIwtRX5CD4AAPwPpraiH8EHAOB6/95ToovnrrQ09uErO+uqHjzx3ykIPgAAV6PL4y4sbgYAuJIxhtDjQnR8AACuc9drW/X86gJLY9+Z0k9npfJ8OKci+AAAXIUuj7sx1QUAcIXqaqa2QMcHAOACgQSej+68UGc0irexGoQTwQcAENXo8uDnmOoCAESlT/eWEnpQCx0fAEDUCSTwfPngMNWvR18gWhF8AABRhS4PToZICwCICos+2mU59JyZchqhxyXo+AAAHI8d1VFXBB8AgGMZY5SR9Zbl8XR53IfgAwBwpEvnrdTmr0ssjf1Dv7aaPryjzRXBCQg+AADHCWRqa2f2cMXEMLXlVixuBgA4xpGKqoA/tUXocTc6PgAARwgk8Cz5w6/Uq21TG6uBUxF8AAARj2fzwC5MdQEAItaObw8RemArOj4AgIgUSOD516QLdG56so3VIFoQfAAAEYcuD4KF4AMAiBjXLVytFV98Z3k8oQenQvABAESEQLo8/75viE7z8isNp8bfEgBAWB2pqFLHe9+2PJ4uD/xB8AEAhE0gXR6J0AP/EXwAAGERSOj58sFhql+PJ7LAf/ytAQCE1PvbiwP+1BahB1bR8QEAhEwggefXmc301xt62VgN3IjgAwAIOmOMMrLesjx+V85weTxsLorAEXwAAEE1/bWtemF1geXxLGCGnQg+AICgCWRq6/mxvdS3fTMbqwEIPgCAIDhWVa32dy21PJ4uD4KF4AMAsBXP5kEkI/gAAGwTSOj56M4LdUajeBurAWoj+AAAAlZcdlQ9H3zP8ni6PAgVgg8AICBMbcFJCD4AAMsCCT2b7x2s5IYNbKwGODWCDwDAb//ZV6phT6ywPJ4uD8KF4AMA8AtTW3Aygg8AoE6qq43aTre+7cTO7OGKiWHbCYQX29sCAE7pv1fsDCj07J49gtCDiEDHBwBwUoFMbf2u2xn608iu9hUDBIjgAwA4rqOVVepwz9uWx7OjOiIRwQcAUAsLmBGtCD4AgBoCCT3Th3fQH/q1s7EawF4EHwCAJGnFF9/quoVrLI9nagtOQPABADC1Bdcg+ACAiwX6bJ5XJ/TRea0a21gREFwEHwBwqYnPb9CbW/dZHs/UFpyI4AMALsTUFtyK4AMALvJDxTGdfe87lsd/fOeFSmsUb2NFQGg5ZsuKnJwcnX/++UpMTFTz5s112WWXafv27TWuMcZo5syZSktLU3x8vAYMGKBt27aFqWIAiCxt7nwzoNCze/YIQg8czzHBZ/ny5Zo4caJWrVql/Px8HTt2TIMHD9bhw4d918yZM0ePPfaY5s2bp7Vr1yo1NVWDBg1SWVlZGCsHgPBjagv4kccYY8JdhBXffvutmjdvruXLl6tfv34yxigtLU1TpkzRHXfcIUkqLy9XSkqKHnroId144411et/S0lIlJyerpKRESUlJwfwWACDo9pUcUe+cZZbHr57+G6UkxdlYERAcdf397dg1PiUlJZKkJk2aSJJ27dqloqIiDR482HeN1+tV//799fHHH58w+JSXl6u8vNz3dWlpaRCrBoDQocsD1OaYqa6fM8Zo6tSpuuCCC9SpUydJUlFRkSQpJSWlxrUpKSm+c8eTk5Oj5ORk36tly5bBKxwAQqC62hB6gBNwZMdn0qRJ2rJli1auXFnr3C+fKWGMOelzJrKysjR16lTf16WlpYQfAI71wBufauHKXZbH78werpgYns2D6OW44HPzzTfrn//8pz788EOlp6f7jqempkr6sfPTokUL3/Hi4uJaXaCf83q98nq9wSsYAEKELg9wao6Z6jLGaNKkSXr11Ve1bNkyZWRk1DifkZGh1NRU5efn+45VVFRo+fLl6tOnT6jLBYCQOVR+LKDQ07JJPKEHruGYjs/EiRP1wgsv6B//+IcSExN963aSk5MVHx8vj8ejKVOmKDs7W5mZmcrMzFR2drYaNmyoUaNGhbl6AAiOQLs8O7KHqx5TW3ARxwSfBQsWSJIGDBhQ4/iiRYs0evRoSdK0adN05MgRTZgwQQcPHlSvXr2Ul5enxMTEEFcLAMHH1BbgP8c+xydYeI4PgEi389tDuvDR5ZbH33/pOfo/vdvYVxAQAaL+OT4A4EaBdnnYUR1uR/ABAAcwxigj662A3oOpLYDgAwAR74PtxRq9aK3l8W/cfIE6nZFsY0WAcxF8ACCCsYAZsJdjnuMDAG5yrKqa0AMEAR0fAIgwT7z7hf707ueWx39854VKaxRvY0VA9CD4AEAEocsDBBdTXQAQAarYUR0ICTo+ABBmU5ds0qsb91gev/GeQWqcEGtjRUD0IvgAQBjR5QFCi6kuAAiDSj61BYQFHR8ACLFAA89nDwxVXIN6NlUDuAvBBwBCiC4PEF5MdQFACOz49lBAoeeijs0JPYAN6PgAQJAF2uXZmT1cMTHsqA7YgeADAEFSXW3Udjo7qgORhOADAEHw23krteXrEsvjF1x7noad28LGigBIBB8AsB1TW0DkYnEzANjErh3VCT1A8NDxAQAbBBp43pr8a52dlmRTNQBOhOADAAHi2TyAc/g91fXss8/qzTf/9x/5tGnT1KhRI/Xp00dfffWVrcUBQCQrLj1K6AEcxu/gk52drfj4eEnSJ598onnz5mnOnDlq1qyZbr31VtsLBIBI1ObON9Uz+z3L4z+4fQChBwgDv6e6CgsL1b59e0nS66+/riuvvFJ/+MMf1LdvXw0YMMDu+gAg4gTa5dmVM1weDwuYgXDwu+Nz2mmnaf/+/ZKkvLw8XXTRRZKkuLg4HTlyxN7qACCC7P3+iC1TW4QeIHz87vgMGjRIY8eOVbdu3fT5559rxIgfW7Xbtm1TmzZt7K4PACJCoIFnwz2D1CQh1qZqAFjld8fnySefVO/evfXtt9/qlVdeUdOmTSVJ69ev1zXXXGN7gQAQbnZ0eQg9QGTwGGOMPwMKCgqUnp6umJiamckYo8LCQrVq1crWAkOttLRUycnJKikpUVISz9QA3Gxz4fe69MmPLI9v3/w0vTu1v40VATiRuv7+9nuqKyMjQ/v27VPz5s1rHD9w4IAyMjJUVVXlf7UAEGEC7fJ89sBQxTWoZ1M1AOzid/A5UYPo0KFDiouLC7ggAAgnY4wysthRHYhWdQ4+U6dOlSR5PB7de++9atiwoe9cVVWVVq9era5du9peIACEyvLPv9X1/2+N5fFj+rbRjEvOsbEiAHarc/DZuHGjpB//a2jr1q2Kjf3fhXqxsbHq0qWLbr/9dvsrBIAQ4AnMgDvUOfi8//77kqQxY8boiSeeYOEvgKjA1BbgLn6v8Vm0aJEk6csvv9SOHTvUr18/xcfHyxjDQ7kAOErO0v/oqeU7LY9/Zsz5GnBW81NfCCBi+B18Dhw4oKuuukrvv/++PB6PvvjiC7Vt21Zjx45Vo0aN9OijjwajTgCwFdtOAO7k9wMMp0yZogYNGqigoKDGAueRI0fq7bfftrU4ALBbxbFqtp0AXMzvjk9eXp7eeecdpaen1ziemZmpr776yrbCAMBugQaej+68UGc0irepGgDh4HfwOXz4cI1Oz0++++47eb1eW4oCALvxqS0AkoWprn79+um5557zfe3xeFRdXa2HH35YAwcOtLU4AAjU0coqQg8AH787Pg8//LAGDBigdevWqaKiQtOmTdO2bdt04MABffSR9T1tAMBugQae7bOGylufbSeAaOJ3x+fss8/Wli1b1LNnTw0aNEiHDx/W5Zdfro0bN6pdu3bBqBEA/GZHl4fQA0Qfv3dnj3bszg44W1HJUf0q5z3L489MOU15t7KjOuA0QdudfcuWLcc97vF4FBcXp1atWrHIGUBYBNrl+eLBYWpQz+9GOAAH8Tv4dO3a1ff8ip+aRT9/nkWDBg00cuRIPfXUU+zWDiAk2HYCQF35/Z82r732mjIzM5Wbm6vNmzdr06ZNys3N1VlnnaUXXnhBCxcu1LJly3T33XcHo14AqGHZZ98EFHpu7NeW0AO4iN8dnwcffFBPPPGEhgwZ4jvWuXNnpaen65577tGaNWuUkJCg2267TY888oitxQLAz7HtBAB/+R18tm7dqtatW9c63rp1a23dulXSj9Nh+/btC7w6ADgOprYAWOX3VFeHDh00e/ZsVVRU+I5VVlZq9uzZ6tChgyRpz549SklJsa9KAPgfgU5tPT+2F6EHcDG/Oz5PPvmkfvvb3yo9PV2dO3eWx+PRli1bVFVVpTfeeEOStHPnTk2YMMH2YgG4G09gBhAoS8/xOXTokP72t7/p888/lzFGHTp00KhRo5SYmBiMGkOK5/gAkYepLQCnEpTn+FRWVuqss87SG2+8ofHjxwdcJACcyi2LN+ofm/ZaHr915mAlxjWwsSIATuZX8GnQoIHKy8v5FASAkOBTWwDs5vfi5ptvvlkPPfSQjh07Fox6AECHy4/Zsp6H0APgl/xe3Lx69Wq99957ysvL07nnnquEhIQa51999VXbigPgPoEGni0zByuJqS0AJ+B38GnUqJGuuOKKYNQCwOX41BaAYPM7+CxatCgYdQBwsZIjlepyX15A70HoAVAXUbkN8fz585WRkaG4uDh1795dK1asCHdJAE6gzZ1vBhR6PntgKKEHQJ353fGRpJdffll///vfVVBQUOMJzpK0YcMGWwqzasmSJZoyZYrmz5+vvn376qmnntKwYcP06aefqlWrVmGtDUBNTG0BCDW/Oz5//vOfNWbMGDVv3lwbN25Uz5491bRpU+3cuVPDhg0LRo1+eeyxx3TDDTdo7Nix6tixox5//HG1bNlSCxYsOO715eXlKi0trfECEFzbi8oCCj0eD6EHgDV+B5/58+crNzdX8+bNU2xsrKZNm6b8/HxNnjxZJSUlwaixzioqKrR+/XoNHjy4xvHBgwfr448/Pu6YnJwcJScn+14tW7YMRamAa7W5800NefxDy+M/nzVMu3IIPQCs8Tv4FBQUqE+fPpKk+Ph4lZWVSZKuu+46vfjii/ZW56fvvvtOVVVVtTZITUlJUVFR0XHHZGVlqaSkxPcqLCwMRamA6xhjbJnaiq0flUsTAYSI3z9BUlNTtX//fklS69attWrVKknSrl27ZGHbr6D45UPLjDEnfJCZ1+tVUlJSjRcAe6344tuA9tpqmhDL1BYAW/i9uPnCCy/Uv/71L5133nm64YYbdOutt+rll1/WunXrdPnllwejxjpr1qyZ6tWrV6u7U1xcXKsLBCA0Au3y7MgernoxPIEZgD38Dj533XWXzjjjDEnS+PHj1aRJE61cuVKXXHJJ2Bc3x8bGqnv37srPz9fvfvc73/H8/HxdeumlYawMcB92VAcQiTzGz/mpevXqad++fWrevHmN4/v371fz5s1VVVVla4H+WrJkia677jr95S9/Ue/evZWbm6unn35a27ZtU+vWrU85vq7b2gM4sT+/94Uey//c8vjn/m9P9TvzdBsrAhDt6vr72++Oz4ly0qFDhxQXF+fv29lu5MiR2r9/v+6//37t27dPnTp10ltvvVWn0AMgcDybB0Akq3PwmTp1qqQfFw7fe++9atiwoe9cVVWVVq9era5du9peoBUTJkzQhAkTwl0G4CpHK6vU4Z63A3oPQg+AYKtz8Nm4caOkHzs+W7duVWxsrO9cbGysunTpottvv93+CgFEvEC7PO/d1l/tTj/NpmoA4MTqHHzef/99SdKYMWP0xBNPsP4FgKTAQ8+unOEnfNwEANiN3dkBWHKkokod72VqC4CzWNqkFIC7BdrlWXf3RWp2mtemagCg7gg+APzCp7YAOBmb3gCok2/Lygk9AByPjg+AUwo08Hz2wFDFNahnUzUAYB3BB8BJ0eUBEE2Y6gJwXNv2lgQUeoafm0roARBx6PgAqIUd1QFEK4IPgBqY2gIQzZjqAiBJWvbZNwGFnkkD2xN6AEQ8Oj4A2HYCgGvQ8QFcrLra2DK1RegB4BR0fACX+n8rd+n+Nz61PD7/1n7KTEm0sSIACD6CD+BCLGAG4FZMdQEuYtfUFgA4FR0fwCXOunupyo9VWx7PthMAogHBB3ABujwA8COCDxDFjlRUqeO9b1se/6eRXfS7buk2VgQA4UXwAaIUz+YBgNoIPkAUYmoLAI6P4ANEkeKyo+r54HuWx2+4Z5CaJMTaWBEARBaCDxAl6PIAwKnxHB8gChB6AKBu6PgADrZtb4lG/Hml5fEb7xmkxkxtAXARgg/gUHR5AMB/THUBDmNMYNtOjOzRktADwLXo+AAO8vrGPZqyZJPl8TybB4DbEXwAh2BqCwACR/ABIpwxRhlZb1kev/D6HvpNxxQbKwIA5yL4ABFs2sub9fd1X1seT5cHAGoi+AARiqktALAfwQeIMOXHqnTW3dZ3VF95x0ClN25oY0UAED0IPkAEocsDAMHFc3yACEHoAYDgo+MDhNm3ZeU6/8F3LY/fkT1c9WJ4Ng8A1AXBBwgjujwAEFoEHyBMAgk9c67orP86v6WN1QCAOxB8gBArPPCDfj3nfcvj2XYCAKwj+AAhxNQWAIQXwQcIkUBCzwvjeqlPu2Y2VgMA7kTwAYLs33tKdPHclZbH0+UBAPsQfIAgYmoLACILwQcIgkB3VP/0/iFqGMs/TwCwGz9ZAZu9tK5Qf3x5i+XxdHkAIHgIPoCNApnaOj3Rq7V3XWRjNQCAXyL4ADYIdGqLbScAIDQIPkCApv59k17dsMfyeKa2ACB0CD5AAAKZ2sr+3bka1auVjdUAAE6F4ANYcLSySh3uedvyeLadAIDwIPgAfvpV9nsqKj1qeTxTWwAQPgQfwA+BTG0t/+MAtW6aYGM1AAB/EXyAOvih4pjOvvcdy+Pp8gBAZCD4AKfAthMAED1iwl1AXezevVs33HCDMjIyFB8fr3bt2mnGjBmqqKiocV1BQYEuueQSJSQkqFmzZpo8eXKtawB/BBJ6tt03hNADABHGER2fzz77TNXV1XrqqafUvn17/fvf/9a4ceN0+PBhPfLII5KkqqoqjRgxQqeffrpWrlyp/fv36/rrr5cxRnPnzg3zdwCnKT1aqc4z8yyPJ/AAQGTyGGNMuIuw4uGHH9aCBQu0c+dOSdLSpUt18cUXq7CwUGlpaZKkxYsXa/To0SouLlZSUlKd3re0tFTJyckqKSmp8xhEl0C6PH3bN9XzY39lYzUAgLqo6+9vR3R8jqekpERNmjTxff3JJ5+oU6dOvtAjSUOGDFF5ebnWr1+vgQMHHvd9ysvLVV5e7vu6tLQ0eEUj4gUSer58cJjq13PE7DEAuJYjf0rv2LFDc+fO1fjx433HioqKlJKSUuO6xo0bKzY2VkVFRSd8r5ycHCUnJ/teLVu2DFrdiFy7vzscUOjZPXsEoQcAHCCsP6lnzpwpj8dz0te6detqjNm7d6+GDh2qq666SmPHjq1x7nhPwjXGnPQJuVlZWSopKfG9CgsL7fnm4Bht7nxTAx75wNLYVyf0YT0PADhIWKe6Jk2apKuvvvqk17Rp08b3v/fu3auBAweqd+/eys3NrXFdamqqVq9eXePYwYMHVVlZWasT9HNer1der9f/4uF4ge6oTuABAOcJa/Bp1qyZmjVrVqdr9+zZo4EDB6p79+5atGiRYmJqNqt69+6tBx98UPv27VOLFi0kSXl5efJ6verevbvttcPZ/r2nRBfPXWl5PKEHAJzJEZ/q2rt3r/r3769WrVrpueeeU7169XznUlNTJf34cfauXbsqJSVFDz/8sA4cOKDRo0frsssu8+vj7HyqK/oFspZn872DldywgY3VAADsEFWf6srLy9OXX36pL7/8Uunp6TXO/ZTb6tWrpzfffFMTJkxQ3759FR8fr1GjRvme8wNUVxu1nc7UFgC4mSM6PqFExyc6PZq3XXOXfWlp7CVd0jT3mm42VwQAsFNUdXyAQAQytbUze7hiYk78qUAAgLPw4BFEraOVVQE/m4fQAwDRhY4PolIggWfeqG66uHPaqS8EADgOwQdRJ5DQsytn+EkfeAkAcDamuhA1ikuPBjy1RegBgOhGxwdRIZDA80nWhWqRHG9jNQCASEXwgeMF2uUBALgHU11wrMIDP1gOPb//VStCDwC4EB0fOBILmAEAVhB84DhMbQEArCL4wDFW79yvkbmrLI1d/Idf6Vdtm9pcEQDAaQg+cASmtgAAdiD4IKIZY5SRxY7qAAB7EHwQseYt+0KP5H1uaeyau36j5olxNlcEAHA6gg8iElNbAIBg4Dk+iChV1YZtJwAAQUPHBxHjzLuXquJYtaWxnz0wVHEN6tlcEQAg2hB8EBF4Ng8AIBQIPgirHyqO6ex737E0dvbl5+rqnq1srggAEM0IPggbFjADAEKN4IOwYGoLABAOBB+EVMmRSnW5L8/SWBYwAwACRfBByNDlAQCEG8/xQUgQegAAkYCOD4Lqi2/KNOhPH1oauzN7uGJiWMAMALAPwQdBQ5cHABBpmOqC7YwJfNsJAACCgY4PbHXHy1u0ZF2hpbFMbQEAgo3gA9vQ5QEARDqmuhCwY1XVlkPPpV3TCD0AgJCh44OAXD7/I20o+N7SWLadAACEGsEHljG1BQBwGoIP/Hasqlrt71pqaeyHfxyoVk0b2lwRAAB1Q/CBX+jyAACcjMXNqDNCDwDA6ej44JQOHK7QeQ/kWxrLs3kAAJGE4IOTossDAIgmBB+ckNXQ88chZ2niwPY2VwMAQOAIPqjloy+/07X/vdrSWJ7NAwCIZAQf1MDUFgAgmhF8IOnHHdUzst6yNPbdqf3UvnmizRUBAGA/gg+U/+k3GvfcOktj6fIAAJyE4ONyTG0BANyE4ONSVdVG7aZbm9r6fNYwxdbn2ZcAAOch+LjQI+9s17z3v7Q0li4PAMDJCD4uY3Vqa9rQszRhAM/mAQA4G8HHJQKZ2qLLAwCIFgQfF7j0yY+0ufB7S2MJPQCAaELwiXJWp7b+MbGvurRsZG8xAACEGcEnSpUcqVSX+/IsjaXLAwCIVgSfKHTW3UtVfqza0lhCDwAgmhF8oozVqa3N9w5WcsMGNlcDAEBkIfhEie8OlavHrHctjaXLAwBwC4JPFLDa5ZkwoJ2mDe1gczUAAEQugo/DWQ09O7OHKybGY3M1AABENjZccqhD5ccsh57ds0cQegAAruS44FNeXq6uXbvK4/Fo06ZNNc4VFBTokksuUUJCgpo1a6bJkyeroqIiPIUG0bkz3lGnGe/4Pe5fky5gPQ8AwNUcN9U1bdo0paWlafPmzTWOV1VVacSIETr99NO1cuVK7d+/X9dff72MMZo7d26YqrWf1S7Prpzh8njo8gAA3M1RwWfp0qXKy8vTK6+8oqVLl9Y4l5eXp08//VSFhYVKS0uTJD366KMaPXq0HnzwQSUlJR33PcvLy1VeXu77urS0NHjfQAC+LC7TRY99aGksXR4AAH7kmODzzTffaNy4cXr99dfVsGHDWuc/+eQTderUyRd6JGnIkCEqLy/X+vXrNXDgwOO+b05Oju67776g1W0Hq12ebfcNUYLXMf8XAwAQdI5Y42OM0ejRozV+/Hj16NHjuNcUFRUpJSWlxrHGjRsrNjZWRUVFJ3zvrKwslZSU+F6FhYW21h6I6moT0AJmQg8AADWFNfjMnDlTHo/npK9169Zp7ty5Ki0tVVZW1knf73hrWIwxJ13b4vV6lZSUVOMVCXZ+e0htp7/l97iE2HpMbQEAcAJhbQlMmjRJV1999UmvadOmjWbNmqVVq1bJ6/XWONejRw9de+21evbZZ5WamqrVq1fXOH/w4EFVVlbW6gRFuqxXt+jFNf53nrbPGipv/XpBqAgAgOjgMcaYcBdxKgUFBTUWHe/du1dDhgzRyy+/rF69eik9PV1Lly7VxRdfrK+//lotWrSQJC1ZskTXX3+9iouL69zJKS0tVXJyskpKSkLe/amuNpa6PBILmAEA7lbX39+OWATSqlWrGl+fdtppkqR27dopPT1dkjR48GCdffbZuu666/Twww/rwIEDuv322zVu3LiImb46mc2F3+vSJz/ye9yMS87WmL4ZQagIAIDo44jgUxf16tXTm2++qQkTJqhv376Kj4/XqFGj9Mgjj4S7tFO6csHHWvfVQb/H0eUBAMA/jgw+bdq00fFm6Fq1aqU33ngjDBVZ9/d1hYQeAABCxJHBJxocqajSiD+v0M7vDvs1bsvMwUqKaxCkqgAAiG4EnzD4rKhUQx9f4fc4ujwAAASG4BNi24vK/A49dw3vqHH92gapIgAA3IPgE0KHyo9pyOP+7be1M3u4YmLYXBQAADsQfEJo7rIv/LqeqS0AAOxF8AmR7UVlemr5zjpd+8pNvdW9dZMgVwQAgPsQfEJk6b/31em6XTnDT7q3GAAAsM4Ru7NHg0u6pJ30/O9/1Uq7Z48g9AAAEER0fEJk6pJNJzz3SdaFapEcH7piAABwKTo+ITL4nNRaxy7vdoZ2zx5B6AEAIETo+ITIxIHt1aZpgma//R+NvaCtrvtVaz6mDgBAiBF8QmhE5xYa0blFuMsAAMC1mOoCAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuQfABAACuUT/cBUQaY4wkqbS0NMyVAACAuvrp9/ZPv8dPhODzC2VlZZKkli1bhrkSAADgr7KyMiUnJ5/wvMecKhq5THV1tfbu3avExER5PJ5wl3NSpaWlatmypQoLC5WUlBTucqIa9zp0uNehwX0OHe51aBhjVFZWprS0NMXEnHglDx2fX4iJiVF6enq4y/BLUlIS/5hChHsdOtzr0OA+hw73OvhO1un5CYubAQCAaxB8AACAaxB8HMzr9WrGjBnyer3hLiXqca9Dh3sdGtzn0OFeRxYWNwMAANeg4wMAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4ONw5eXl6tq1qzwejzZt2lTjXEFBgS655BIlJCSoWbNmmjx5sioqKsJTqEPt3r1bN9xwgzIyMhQfH6927dppxowZte4j99oe8+fPV0ZGhuLi4tS9e3etWLEi3CU5Wk5Ojs4//3wlJiaqefPmuuyyy7R9+/Ya1xhjNHPmTKWlpSk+Pl4DBgzQtm3bwlRx9MjJyZHH49GUKVN8x7jXkYHg43DTpk1TWlpareNVVVUaMWKEDh8+rJUrV2rx4sV65ZVXdNttt4WhSuf67LPPVF1draeeekrbtm3Tn/70J/3lL3/R9OnTfddwr+2xZMkSTZkyRXfddZc2btyoX//61xo2bJgKCgrCXZpjLV++XBMnTtSqVauUn5+vY8eOafDgwTp8+LDvmjlz5uixxx7TvHnztHbtWqWmpmrQoEG+fQvhv7Vr1yo3N1edO3eucZx7HSEMHOutt94yHTp0MNu2bTOSzMaNG2uci4mJMXv27PEde/HFF43X6zUlJSVhqDZ6zJkzx2RkZPi+5l7bo2fPnmb8+PE1jnXo0MHceeedYaoo+hQXFxtJZvny5cYYY6qrq01qaqqZPXu275qjR4+a5ORk85e//CVcZTpaWVmZyczMNPn5+aZ///7mlltuMcZwryMJHR+H+uabbzRu3Dj99a9/VcOGDWud/+STT9SpU6ca3aAhQ4aovLxc69evD2WpUaekpERNmjTxfc29DlxFRYXWr1+vwYMH1zg+ePBgffzxx2GqKvqUlJRIku/v765du1RUVFTjvnu9XvXv35/7btHEiRM1YsQIXXTRRTWOc68jB5uUOpAxRqNHj9b48ePVo0cP7d69u9Y1RUVFSklJqXGscePGio2NVVFRUYgqjT47duzQ3Llz9eijj/qOca8D991336mqqqrWfUxJSeEe2sQYo6lTp+qCCy5Qp06dJMl3b49337/66quQ1+h0ixcv1oYNG7R27dpa57jXkYOOTwSZOXOmPB7PSV/r1q3T3LlzVVpaqqysrJO+n8fjqXXMGHPc425T13v9c3v37tXQoUN11VVXaezYsTXOca/t8cv7xT20z6RJk7Rlyxa9+OKLtc5x3wNXWFioW265RX/7298UFxd3wuu41+FHxyeCTJo0SVdfffVJr2nTpo1mzZqlVatW1dr3pUePHrr22mv17LPPKjU1VatXr65x/uDBg6qsrKz1XxxuVNd7/ZO9e/dq4MCB6t27t3Jzc2tcx70OXLNmzVSvXr1a3Z3i4mLuoQ1uvvlm/fOf/9SHH36o9PR03/HU1FRJP3YjWrRo4TvOffff+vXrVVxcrO7du/uOVVVV6cMPP9S8efN8n6bjXkeAMK4vgkVfffWV2bp1q+/1zjvvGEnm5ZdfNoWFhcaY/11wu3fvXt+4xYsXs+DWgq+//tpkZmaaq6++2hw7dqzWee61PXr27GluuummGsc6duzI4uYAVFdXm4kTJ5q0tDTz+eefH/d8amqqeeihh3zHysvLWXBrQWlpaY2fy1u3bjU9evQwv//9783WrVu51xGE4BMFdu3aVetTXceOHTOdOnUyv/nNb8yGDRvMu+++a9LT082kSZPCV6gD7dmzx7Rv395ceOGF5uuvvzb79u3zvX7CvbbH4sWLTYMGDczChQvNp59+aqZMmWISEhLM7t27w12aY910000mOTnZfPDBBzX+7v7www++a2bPnm2Sk5PNq6++arZu3WquueYa06JFC1NaWhrGyqPDzz/VZQz3OlIQfKLA8YKPMT92hkaMGGHi4+NNkyZNzKRJk8zRo0fDU6RDLVq0yEg67uvnuNf2ePLJJ03r1q1NbGysOe+883wfu4Y1J/q7u2jRIt811dXVZsaMGSY1NdV4vV7Tr18/s3Xr1vAVHUV+GXy415HBY4wxYZhhAwAACDk+1QUAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4AMAAFyD4AMgasycOVNdu3YNyns/88wzatSoUVDeG0DoEHwAAIBrEHwARIzy8nJNnjxZzZs3V1xcnC644AKtXbtW0vE7Lq+//ro8Ho/v/H333afNmzfL4/HI4/HomWeekSR5PB4tWLBAw4YNU3x8vDIyMvTSSy/53ueDDz6Qx+PR999/7zu2adMmeTwe7d69Wx988IHGjBmjkpIS33vPnDlTkjR//nxlZmYqLi5OKSkpuvLKK4N2fwAEjuADIGJMmzZNr7zyip599llt2LBB7du315AhQ3TgwIFTjh05cqRuu+02nXPOOdq3b5/27dunkSNH+s7fc889uuKKK7R582b9/ve/1zXXXKP//Oc/daqrT58+evzxx5WUlOR779tvv13r1q3T5MmTdf/992v79u16++231a9fP8vfP4Dgqx/uAgBAkg4fPqwFCxbomWee0bBhwyRJTz/9tPLz87Vw4UKdfvrpJx0fHx+v0047TfXr11dqamqt81dddZXGjh0rSXrggQeUn5+vuXPnav78+aesLTY2VsnJyfJ4PDXeu6CgQAkJCbr44ouVmJio1q1bq1u3bv582wBCjI4PgIiwY8cOVVZWqm/fvr5jDRo0UM+ePevcmTmZ3r171/o60PcdNGiQWrdurbZt2+q6667T888/rx9++CGg9wQQXAQfABHBGCNJvjU7Pz/u8XgUExPju+YnlZWVAf2ZP/1ZMTExNWqo63snJiZqw4YNevHFF9WiRQvde++96tKlS421QgAiC8EHQERo3769YmNjtXLlSt+xyspKrVu3Th07dtTpp5+usrIyHT582Hd+06ZNNd4jNjZWVVVVx33/VatW1fq6Q4cOkuSbRtu3b5/f712/fn1ddNFFmjNnjrZs2aLdu3dr2bJlp/6GAYQFa3wARISEhATddNNN+uMf/6gmTZqoVatWmjNnjn744QfdcMMNMsaoYcOGmj59um6++WatWbPG96mtn7Rp00a7du3Spk2blJ6ersTERHm9XknSSy+9pB49euiCCy7Q888/rzVr1mjhwoWSfgxdLVu21MyZMzVr1ix98cUXevTRR2u996FDh/Tee++pS5cuatiwoZYtW6adO3eqX79+aty4sd566y1VV1frrLPOCsk9A2CBAYAIceTIEXPzzTebZs2aGa/Xa/r27WvWrFnjO//aa6+Z9u3bm7i4OHPxxReb3Nxc8/MfY0ePHjVXXHGFadSokZFkFi1aZIwxRpJ58sknzaBBg4zX6zWtW7c2L774Yo0/e+XKlebcc881cXFx5te//rV56aWXjCSza9cu3zXjx483TZs2NZLMjBkzzIoVK0z//v1N48aNTXx8vOncubNZsmRJUO8RgMB4jPnFpDkARBmPx6PXXntNl112WbhLARBmrPEBAACuQfABAACuweJmAFGPGX0AP6HjAwAAXIPgAwAAXIPgAwAAXIPgAwAAXIPgAwAAXIPgAwAAXIPgAwAAXIPgAwAAXOP/A7d/+/DW0FXhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usamos np.squeeze para poder ajustar los datos a lo que espera plt.plot\n",
    "plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()\n",
    "\n",
    "# La idea de haber usado TensorFlow para el mismo ejercicio y poder comparar las líneas de código necesarias\n",
    "# para obtener el mismo resultado.  Muchas menos en TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4be58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f988a5",
   "metadata": {},
   "source": [
    "## **2. Cambia el número de observaciones a 1.000.000 y comprueba qué sucede**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8f098",
   "metadata": {},
   "source": [
    "#### **Generamos los datos (1_000_000)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba37495",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 1_000_000\n",
    "\n",
    "np.random.seed(123)\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# Hasta aquí hemos generado los mismos datos, ahora los guardamos\n",
    "np.savez('data/data_1_000_000', inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d0fd9",
   "metadata": {},
   "source": [
    "#### **Resloviendo con Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841eff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el conjunto de datos\n",
    "training_data = np.load('data/data_1_000_000.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e68789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 750us/step - loss: 0.6661\n",
      "Epoch 2/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 688us/step - loss: 0.3441\n",
      "Epoch 3/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 697us/step - loss: 0.3444\n",
      "Epoch 4/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 594us/step - loss: 0.3440\n",
      "Epoch 5/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 632us/step - loss: 0.3434\n",
      "Epoch 6/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 983us/step - loss: 0.3438\n",
      "Epoch 7/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 768us/step - loss: 0.3436\n",
      "Epoch 8/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 841us/step - loss: 0.3438\n",
      "Epoch 9/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610us/step - loss: 0.3442\n",
      "Epoch 10/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 587us/step - loss: 0.3435\n",
      "Epoch 11/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 734us/step - loss: 0.3436\n",
      "Epoch 12/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 799us/step - loss: 0.3434\n",
      "Epoch 13/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 582us/step - loss: 0.3437\n",
      "Epoch 14/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 635us/step - loss: 0.3437\n",
      "Epoch 15/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 605us/step - loss: 0.3439\n",
      "Epoch 16/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498us/step - loss: 0.3433\n",
      "Epoch 17/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 653us/step - loss: 0.3441\n",
      "Epoch 18/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 779us/step - loss: 0.3445\n",
      "Epoch 19/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 688us/step - loss: 0.3441\n",
      "Epoch 20/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 819us/step - loss: 0.3439\n",
      "Epoch 21/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 698us/step - loss: 0.3444\n",
      "Epoch 22/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 765us/step - loss: 0.3435\n",
      "Epoch 23/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 702us/step - loss: 0.3439\n",
      "Epoch 24/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 536us/step - loss: 0.3440\n",
      "Epoch 25/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 763us/step - loss: 0.3440\n",
      "Epoch 26/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 813us/step - loss: 0.3435\n",
      "Epoch 27/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 623us/step - loss: 0.3444\n",
      "Epoch 28/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 832us/step - loss: 0.3439\n",
      "Epoch 29/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 752us/step - loss: 0.3440\n",
      "Epoch 30/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 615us/step - loss: 0.3443\n",
      "Epoch 31/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 559us/step - loss: 0.3440\n",
      "Epoch 32/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 707us/step - loss: 0.3434\n",
      "Epoch 33/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 851us/step - loss: 0.3446\n",
      "Epoch 34/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 527us/step - loss: 0.3442\n",
      "Epoch 35/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 540us/step - loss: 0.3446\n",
      "Epoch 36/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 633us/step - loss: 0.3433\n",
      "Epoch 37/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 554us/step - loss: 0.3441\n",
      "Epoch 38/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 547us/step - loss: 0.3432\n",
      "Epoch 39/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 521us/step - loss: 0.3433\n",
      "Epoch 40/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 494us/step - loss: 0.3435\n",
      "Epoch 41/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 507us/step - loss: 0.3437\n",
      "Epoch 42/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 513us/step - loss: 0.3435\n",
      "Epoch 43/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 516us/step - loss: 0.3440\n",
      "Epoch 44/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 550us/step - loss: 0.3435\n",
      "Epoch 45/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 550us/step - loss: 0.3440\n",
      "Epoch 46/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 518us/step - loss: 0.3431\n",
      "Epoch 47/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 484us/step - loss: 0.3440\n",
      "Epoch 48/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 509us/step - loss: 0.3438\n",
      "Epoch 49/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 590us/step - loss: 0.3433\n",
      "Epoch 50/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 568us/step - loss: 0.3440\n",
      "Epoch 51/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 514us/step - loss: 0.3443\n",
      "Epoch 52/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 506us/step - loss: 0.3439\n",
      "Epoch 53/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 508us/step - loss: 0.3438\n",
      "Epoch 54/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 621us/step - loss: 0.3442\n",
      "Epoch 55/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 626us/step - loss: 0.3440\n",
      "Epoch 56/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 700us/step - loss: 0.3443\n",
      "Epoch 57/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 732us/step - loss: 0.3444\n",
      "Epoch 58/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 587us/step - loss: 0.3439\n",
      "Epoch 59/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607us/step - loss: 0.3434\n",
      "Epoch 60/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 595us/step - loss: 0.3436\n",
      "Epoch 61/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 724us/step - loss: 0.3440\n",
      "Epoch 62/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 639us/step - loss: 0.3444\n",
      "Epoch 63/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 661us/step - loss: 0.3440\n",
      "Epoch 64/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 579us/step - loss: 0.3443\n",
      "Epoch 65/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 590us/step - loss: 0.3438\n",
      "Epoch 66/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 852us/step - loss: 0.3436\n",
      "Epoch 67/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607us/step - loss: 0.3440\n",
      "Epoch 68/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 578us/step - loss: 0.3439\n",
      "Epoch 69/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 525us/step - loss: 0.3438\n",
      "Epoch 70/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 502us/step - loss: 0.3439\n",
      "Epoch 71/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 618us/step - loss: 0.3441\n",
      "Epoch 72/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 506us/step - loss: 0.3443\n",
      "Epoch 73/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 755us/step - loss: 0.3437\n",
      "Epoch 74/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 718us/step - loss: 0.3440\n",
      "Epoch 75/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 527us/step - loss: 0.3436\n",
      "Epoch 76/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 637us/step - loss: 0.3439\n",
      "Epoch 77/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 600us/step - loss: 0.3445\n",
      "Epoch 78/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 558us/step - loss: 0.3441\n",
      "Epoch 79/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 862us/step - loss: 0.3438\n",
      "Epoch 80/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 632us/step - loss: 0.3441\n",
      "Epoch 81/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 613us/step - loss: 0.3442\n",
      "Epoch 82/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 521us/step - loss: 0.3436\n",
      "Epoch 83/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 844us/step - loss: 0.3442\n",
      "Epoch 84/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 866us/step - loss: 0.3438\n",
      "Epoch 85/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 636us/step - loss: 0.3439\n",
      "Epoch 86/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 855us/step - loss: 0.3440\n",
      "Epoch 87/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 613us/step - loss: 0.3446\n",
      "Epoch 88/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 573us/step - loss: 0.3437\n",
      "Epoch 89/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 595us/step - loss: 0.3436\n",
      "Epoch 90/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 588us/step - loss: 0.3441\n",
      "Epoch 91/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 609us/step - loss: 0.3443\n",
      "Epoch 92/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 668us/step - loss: 0.3435\n",
      "Epoch 93/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 655us/step - loss: 0.3439\n",
      "Epoch 94/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 663us/step - loss: 0.3441\n",
      "Epoch 95/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 677us/step - loss: 0.3434\n",
      "Epoch 96/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 568us/step - loss: 0.3434\n",
      "Epoch 97/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 538us/step - loss: 0.3442\n",
      "Epoch 98/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 618us/step - loss: 0.3434\n",
      "Epoch 99/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 749us/step - loss: 0.3447\n",
      "Epoch 100/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 557us/step - loss: 0.3439\n",
      "Epoch 101/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 544us/step - loss: 0.3437\n",
      "Epoch 102/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 529us/step - loss: 0.3439\n",
      "Epoch 103/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 722us/step - loss: 0.3434\n",
      "Epoch 104/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 540us/step - loss: 0.3437\n",
      "Epoch 105/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 570us/step - loss: 0.3441\n",
      "Epoch 106/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 771us/step - loss: 0.3446\n",
      "Epoch 107/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 869us/step - loss: 0.3435\n",
      "Epoch 108/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 596us/step - loss: 0.3437\n",
      "Epoch 109/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 569us/step - loss: 0.3434\n",
      "Epoch 110/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 928us/step - loss: 0.3431\n",
      "Epoch 111/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 890us/step - loss: 0.3433\n",
      "Epoch 112/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 771us/step - loss: 0.3442\n",
      "Epoch 113/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 815us/step - loss: 0.3437\n",
      "Epoch 114/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 547us/step - loss: 0.3438\n",
      "Epoch 115/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499us/step - loss: 0.3436\n",
      "Epoch 116/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 693us/step - loss: 0.3441\n",
      "Epoch 117/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 763us/step - loss: 0.3435\n",
      "Epoch 118/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 641us/step - loss: 0.3433\n",
      "Epoch 119/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 500us/step - loss: 0.3444\n",
      "Epoch 120/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 504us/step - loss: 0.3433\n",
      "Epoch 121/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 668us/step - loss: 0.3437\n",
      "Epoch 122/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 519us/step - loss: 0.3433\n",
      "Epoch 123/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 504us/step - loss: 0.3436\n",
      "Epoch 124/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 525us/step - loss: 0.3433\n",
      "Epoch 125/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 569us/step - loss: 0.3437\n",
      "Epoch 126/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 618us/step - loss: 0.3439\n",
      "Epoch 127/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 593us/step - loss: 0.3436\n",
      "Epoch 128/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 551us/step - loss: 0.3443\n",
      "Epoch 129/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 551us/step - loss: 0.3442\n",
      "Epoch 130/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 517us/step - loss: 0.3432\n",
      "Epoch 131/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 560us/step - loss: 0.3437\n",
      "Epoch 132/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 544us/step - loss: 0.3438\n",
      "Epoch 133/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 552us/step - loss: 0.3442\n",
      "Epoch 134/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 563us/step - loss: 0.3437\n",
      "Epoch 135/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 547us/step - loss: 0.3439\n",
      "Epoch 136/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 529us/step - loss: 0.3439\n",
      "Epoch 137/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 547us/step - loss: 0.3437\n",
      "Epoch 138/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 537us/step - loss: 0.3441\n",
      "Epoch 139/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 524us/step - loss: 0.3440\n",
      "Epoch 140/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 548us/step - loss: 0.3445\n",
      "Epoch 141/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 525us/step - loss: 0.3443\n",
      "Epoch 142/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 508us/step - loss: 0.3431\n",
      "Epoch 143/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 532us/step - loss: 0.3434\n",
      "Epoch 144/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 534us/step - loss: 0.3442\n",
      "Epoch 145/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 543us/step - loss: 0.3440\n",
      "Epoch 146/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 550us/step - loss: 0.3435\n",
      "Epoch 147/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 551us/step - loss: 0.3434\n",
      "Epoch 148/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 544us/step - loss: 0.3444\n",
      "Epoch 149/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 514us/step - loss: 0.3445\n",
      "Epoch 150/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 524us/step - loss: 0.3442\n",
      "Epoch 151/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 525us/step - loss: 0.3432\n",
      "Epoch 152/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499us/step - loss: 0.3442\n",
      "Epoch 153/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496us/step - loss: 0.3435\n",
      "Epoch 154/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 492us/step - loss: 0.3438\n",
      "Epoch 155/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 508us/step - loss: 0.3446\n",
      "Epoch 156/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 491us/step - loss: 0.3438\n",
      "Epoch 157/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496us/step - loss: 0.3438\n",
      "Epoch 158/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496us/step - loss: 0.3439\n",
      "Epoch 159/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 501us/step - loss: 0.3437\n",
      "Epoch 160/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 508us/step - loss: 0.3443\n",
      "Epoch 161/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496us/step - loss: 0.3438\n",
      "Epoch 162/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 491us/step - loss: 0.3444\n",
      "Epoch 163/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498us/step - loss: 0.3440\n",
      "Epoch 164/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496us/step - loss: 0.3434\n",
      "Epoch 165/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 504us/step - loss: 0.3434\n",
      "Epoch 166/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 500us/step - loss: 0.3441\n",
      "Epoch 167/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 496us/step - loss: 0.3437\n",
      "Epoch 168/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499us/step - loss: 0.3431\n",
      "Epoch 169/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498us/step - loss: 0.3440\n",
      "Epoch 170/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499us/step - loss: 0.3438\n",
      "Epoch 171/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 495us/step - loss: 0.3439\n",
      "Epoch 172/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 535us/step - loss: 0.3436\n",
      "Epoch 173/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 749us/step - loss: 0.3437\n",
      "Epoch 174/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 603us/step - loss: 0.3437\n",
      "Epoch 175/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 673us/step - loss: 0.3440\n",
      "Epoch 176/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 582us/step - loss: 0.3440\n",
      "Epoch 177/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 730us/step - loss: 0.3438\n",
      "Epoch 178/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 513us/step - loss: 0.3434\n",
      "Epoch 179/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 602us/step - loss: 0.3438\n",
      "Epoch 180/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 527us/step - loss: 0.3443\n",
      "Epoch 181/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 491us/step - loss: 0.3443\n",
      "Epoch 182/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 495us/step - loss: 0.3432\n",
      "Epoch 183/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 495us/step - loss: 0.3439\n",
      "Epoch 184/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 706us/step - loss: 0.3442\n",
      "Epoch 185/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 587us/step - loss: 0.3438\n",
      "Epoch 186/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 601us/step - loss: 0.3444\n",
      "Epoch 187/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 600us/step - loss: 0.3436\n",
      "Epoch 188/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 597us/step - loss: 0.3438\n",
      "Epoch 189/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 587us/step - loss: 0.3437\n",
      "Epoch 190/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 594us/step - loss: 0.3436\n",
      "Epoch 191/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 584us/step - loss: 0.3437\n",
      "Epoch 192/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 588us/step - loss: 0.3435\n",
      "Epoch 193/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 596us/step - loss: 0.3433\n",
      "Epoch 194/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 599us/step - loss: 0.3436\n",
      "Epoch 195/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 604us/step - loss: 0.3438\n",
      "Epoch 196/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 607us/step - loss: 0.3438\n",
      "Epoch 197/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 593us/step - loss: 0.3441\n",
      "Epoch 198/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 600us/step - loss: 0.3439\n",
      "Epoch 199/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 603us/step - loss: 0.3442\n",
      "Epoch 200/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 632us/step - loss: 0.3439\n",
      "Epoch 201/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 633us/step - loss: 0.3444\n",
      "Epoch 202/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 627us/step - loss: 0.3439\n",
      "Epoch 203/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 635us/step - loss: 0.3438\n",
      "Epoch 204/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 634us/step - loss: 0.3444\n",
      "Epoch 205/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 644us/step - loss: 0.3438\n",
      "Epoch 206/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 640us/step - loss: 0.3439\n",
      "Epoch 207/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 636us/step - loss: 0.3438\n",
      "Epoch 208/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 617us/step - loss: 0.3436\n",
      "Epoch 209/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 644us/step - loss: 0.3446\n",
      "Epoch 210/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 618us/step - loss: 0.3435\n",
      "Epoch 211/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 615us/step - loss: 0.3438\n",
      "Epoch 212/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 628us/step - loss: 0.3442\n",
      "Epoch 213/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 608us/step - loss: 0.3440\n",
      "Epoch 214/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 612us/step - loss: 0.3440\n",
      "Epoch 215/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 635us/step - loss: 0.3438\n",
      "Epoch 216/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 641us/step - loss: 0.3437\n",
      "Epoch 217/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 620us/step - loss: 0.3438\n",
      "Epoch 218/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 598us/step - loss: 0.3434\n",
      "Epoch 219/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 14ms/step - loss: 0.3435\n",
      "Epoch 220/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 673us/step - loss: 0.3436\n",
      "Epoch 221/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 594us/step - loss: 0.3442\n",
      "Epoch 222/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 592us/step - loss: 0.3433\n",
      "Epoch 223/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 587us/step - loss: 0.3434\n",
      "Epoch 224/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 591us/step - loss: 0.3433\n",
      "Epoch 225/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 586us/step - loss: 0.3438\n",
      "Epoch 226/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 610us/step - loss: 0.3443\n",
      "Epoch 227/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 704us/step - loss: 0.3438\n",
      "Epoch 228/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 629us/step - loss: 0.3440\n",
      "Epoch 229/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 543us/step - loss: 0.3434\n",
      "Epoch 230/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 552us/step - loss: 0.3439\n",
      "Epoch 231/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 535us/step - loss: 0.3440\n",
      "Epoch 232/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 539us/step - loss: 0.3438\n",
      "Epoch 233/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 540us/step - loss: 0.3448\n",
      "Epoch 234/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 548us/step - loss: 0.3438\n",
      "Epoch 235/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 835us/step - loss: 0.3439\n",
      "Epoch 236/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 735us/step - loss: 0.3445\n",
      "Epoch 237/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 773us/step - loss: 0.3434\n",
      "Epoch 238/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 817us/step - loss: 0.3431\n",
      "Epoch 239/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 895us/step - loss: 0.3443\n",
      "Epoch 240/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 776us/step - loss: 0.3436\n",
      "Epoch 241/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 733us/step - loss: 0.3436\n",
      "Epoch 242/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 679us/step - loss: 0.3438\n",
      "Epoch 243/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 775us/step - loss: 0.3439\n",
      "Epoch 244/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 716us/step - loss: 0.3437\n",
      "Epoch 245/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 690us/step - loss: 0.3440\n",
      "Epoch 246/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 737us/step - loss: 0.3441\n",
      "Epoch 247/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 724us/step - loss: 0.3439\n",
      "Epoch 248/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 704us/step - loss: 0.3437\n",
      "Epoch 249/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 700us/step - loss: 0.3436\n",
      "Epoch 250/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 680us/step - loss: 0.3439\n",
      "Epoch 251/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 648us/step - loss: 0.3440\n",
      "Epoch 252/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 721us/step - loss: 0.3439\n",
      "Epoch 253/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 660us/step - loss: 0.3439\n",
      "Epoch 254/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 673us/step - loss: 0.3437\n",
      "Epoch 255/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 932us/step - loss: 0.3442\n",
      "Epoch 256/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 775us/step - loss: 0.3444\n",
      "Epoch 257/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 870us/step - loss: 0.3439\n",
      "Epoch 258/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1ms/step - loss: 0.3445\n",
      "Epoch 259/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 784us/step - loss: 0.3440\n",
      "Epoch 260/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 733us/step - loss: 0.3440\n",
      "Epoch 261/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 723us/step - loss: 0.3443\n",
      "Epoch 262/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 932us/step - loss: 0.3437\n",
      "Epoch 263/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 734us/step - loss: 0.3442\n",
      "Epoch 264/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 698us/step - loss: 0.3436\n",
      "Epoch 265/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 709us/step - loss: 0.3439\n",
      "Epoch 266/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 630us/step - loss: 0.3440\n",
      "Epoch 267/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 690us/step - loss: 0.3439\n",
      "Epoch 268/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 675us/step - loss: 0.3433\n",
      "Epoch 269/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 691us/step - loss: 0.3436\n",
      "Epoch 270/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 706us/step - loss: 0.3438\n",
      "Epoch 271/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 639us/step - loss: 0.3434\n",
      "Epoch 272/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 684us/step - loss: 0.3436\n",
      "Epoch 273/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 655us/step - loss: 0.3445\n",
      "Epoch 274/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 626us/step - loss: 0.3437\n",
      "Epoch 275/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 678us/step - loss: 0.3437\n",
      "Epoch 276/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 631us/step - loss: 0.3438\n",
      "Epoch 277/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 621us/step - loss: 0.3436\n",
      "Epoch 278/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 646us/step - loss: 0.3442\n",
      "Epoch 279/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 675us/step - loss: 0.3439\n",
      "Epoch 280/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 661us/step - loss: 0.3447\n",
      "Epoch 281/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 636us/step - loss: 0.3436\n",
      "Epoch 282/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 649us/step - loss: 0.3440\n",
      "Epoch 283/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 649us/step - loss: 0.3443\n",
      "Epoch 284/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 689us/step - loss: 0.3445\n",
      "Epoch 285/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 709us/step - loss: 0.3441\n",
      "Epoch 286/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 671us/step - loss: 0.3441\n",
      "Epoch 287/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 647us/step - loss: 0.3436\n",
      "Epoch 288/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 647us/step - loss: 0.3434\n",
      "Epoch 289/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 669us/step - loss: 0.3437\n",
      "Epoch 290/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 831us/step - loss: 0.3436\n",
      "Epoch 291/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 790us/step - loss: 0.3438\n",
      "Epoch 292/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 872us/step - loss: 0.3442\n",
      "Epoch 293/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 846us/step - loss: 0.3431\n",
      "Epoch 294/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 693us/step - loss: 0.3440\n",
      "Epoch 295/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 817us/step - loss: 0.3434\n",
      "Epoch 296/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 758us/step - loss: 0.3437\n",
      "Epoch 297/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 612us/step - loss: 0.3434\n",
      "Epoch 298/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 561us/step - loss: 0.3440\n",
      "Epoch 299/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 554us/step - loss: 0.3437\n",
      "Epoch 300/300\n",
      "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 590us/step - loss: 0.3440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f981e7a0580>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declaramos una variable donde guardaremos el total de variables del modelo\n",
    "# En nuestro caso 2 xs y zs\n",
    "#input_size = 2\n",
    "\n",
    "# Declaramos el total de variables de salida del modelo\n",
    "# En nuestro caso 1\n",
    "output_size = 1\n",
    "\n",
    "# Definimos nuestro modelo como secuencial.\n",
    "# En esta etapa no hay datos involucrados, sólo definimos lo que queremos hacer.\n",
    "model = tf.keras.Sequential([\n",
    "                            # Cada capa se lista aquí\n",
    "                            # Definimos las características de la red\n",
    "                            # Dense, nos permite definir la operación matemática a ejecutar xw + b\n",
    "                            # Básicamente aplica la operación: output = activation(dot(input, kernel) + bias\n",
    "                            tf.keras.layers.Dense(output_size,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])\n",
    "\n",
    "# Definimos el ratio de aprendizaje\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Definimos la función de pérdida a minimizar.  En este caso el error cuadrático medio\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Definimos el modelo con los datos de entrada y targets.\n",
    "# Ejecutamos 100 ciclos\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb932c6b",
   "metadata": {},
   "source": [
    "#### **Extraemos los pesos y bias**\n",
    "\n",
    "La extracción de pesos y bias del modelo no es necesaria en el proceso de machine learning.  De hecho, no se ejecuta habitualmente, pero en este ejemplo simple nos sirve para comprobar que los resultados son correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "557d6524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.9966922],\n",
       "        [-2.9936044]], dtype=float32),\n",
       " array([5.006704], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La extracción de pesos y biases es muy sencilla\n",
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71ac7e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9966922]\n",
      " [-2.9936044]]\n",
      "[5.006704]\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "bias = model.layers[0].get_weights()[1]\n",
    "print(weights)\n",
    "print(bias)\n",
    "\n",
    "# éste es un modelo simple, pero en otros más complicados podemos tener cientos o miles de valores en estas variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7ee4e",
   "metadata": {},
   "source": [
    "#### **Generar las predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b05f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.5],\n",
       "       [ -1.6],\n",
       "       [ 22.8],\n",
       "       ...,\n",
       "       [ -8.1],\n",
       "       [-11.1],\n",
       "       [-22.8]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos predichos\n",
    "model.predict_on_batch(training_data['inputs']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeda366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos reales\n",
    "training_data['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fc3b9",
   "metadata": {},
   "source": [
    "#### **Pintamos los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79987aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77135b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos np.squeeze para poder ajustar los datos a lo que espera plt.plot\n",
    "plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()\n",
    "\n",
    "# La idea de haber usado TensorFlow para el mismo ejercicio y poder comparar las líneas de código necesarias\n",
    "# para obtener el mismo resultado.  Muchas menos en TensorFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4636a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b038b",
   "metadata": {},
   "source": [
    "## **3. Juega con el ratio de aprendizaje con los siguientes valores: 0.0001, 0.001, 0.1, 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc662c79",
   "metadata": {},
   "source": [
    "#### **Generamos los datos (10_000)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 10_000\n",
    "\n",
    "np.random.seed(123)\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# Hasta aquí hemos generado los mismos datos, ahora los guardamos\n",
    "np.savez('data/data_10_000', inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94557f2d",
   "metadata": {},
   "source": [
    "#### **Resloviendo con Tensorflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de88f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el conjunto de datos\n",
    "training_data = np.load('data/data_10_000.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67918478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Declaramos una variable donde guardaremos el total de variables del modelo\n",
    "# # En nuestro caso 2 xs y zs\n",
    "# #input_size = 2\n",
    "\n",
    "# # Declaramos el total de variables de salida del modelo\n",
    "# # En nuestro caso 1\n",
    "# output_size = 1\n",
    "\n",
    "# # Definimos nuestro modelo como secuencial.\n",
    "# # En esta etapa no hay datos involucrados, sólo definimos lo que queremos hacer.\n",
    "# model = tf.keras.Sequential([\n",
    "#                             # Cada capa se lista aquí\n",
    "#                             # Definimos las características de la red\n",
    "#                             # Dense, nos permite definir la operación matemática a ejecutar xw + b\n",
    "#                             # Básicamente aplica la operación: output = activation(dot(input, kernel) + bias\n",
    "#                             tf.keras.layers.Dense(output_size,\n",
    "#                                                  kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "#                                                  bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "#                                                  )\n",
    "#                             ])\n",
    "\n",
    "# # Definimos el ratio de aprendizaje\n",
    "# custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# # Definimos la función de pérdida a minimizar.  En este caso el error cuadrático medio\n",
    "# model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# # Definimos el modelo con los datos de entrada y targets.\n",
    "# # Ejecutamos 100 ciclos\n",
    "# model.fit(training_data['inputs'], training_data['targets'], epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f85ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data, learning_rate):\n",
    "    # Definimos el tamaño de salida del modelo\n",
    "    output_size = 1\n",
    "\n",
    "    # Definimos el modelo\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(output_size,\n",
    "                              kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                              bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "    ])\n",
    "\n",
    "    # Configuramos el optimizador con el learning rate específico\n",
    "    custom_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compilamos el modelo con la función de pérdida\n",
    "    model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    history = model.fit(training_data['inputs'], training_data['targets'], \n",
    "                        epochs=300, verbose=0)  # Cambiar verbose a 1 para más detalles\n",
    "\n",
    "    # Obtenemos el error final (última pérdida)\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    return final_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4489fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores de learning rate\n",
    "learning_rates = [0.0001, 0.001, 0.1, 1]\n",
    "\n",
    "# Cargamos los datos\n",
    "training_data = np.load('data/data_10_000.npz')\n",
    "\n",
    "# Diccionario para guardar los resultados\n",
    "results = {}\n",
    "\n",
    "# Iteramos sobre los learning rates\n",
    "for lr in learning_rates:\n",
    "    print(f\"Entrenando con learning_rate = {lr}\")\n",
    "    final_loss = train_model(training_data, learning_rate=lr)\n",
    "    results[lr] = final_loss\n",
    "    print(f\"Final loss: {final_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6583be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos los resultados\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results.keys(), results.values(), marker='o')\n",
    "plt.xscale('log')  # Escala logarítmica para visualizar mejor los cambios\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.title('Efecto del Learning Rate en la Pérdida Final')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56093f7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463dc4d",
   "metadata": {},
   "source": [
    "## **4. Prueba cambiar la función de pérdida.  En el ejercicio, minimizamos el error cuadrático medio.  Esta vez minimiza el error absoluto, que es básicamente el sumatorio de los deltas en valor absoluto $$ \\Sigma_i = |y_i-t_i| $$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddff360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el conjunto de datos\n",
    "training_data = np.load('data/data_10_000.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos el total de variables de salida del modelo\n",
    "# En nuestro caso 1\n",
    "output_size = 1\n",
    "\n",
    "# Definimos nuestro modelo como secuencial.\n",
    "# En esta etapa no hay datos involucrados, sólo definimos lo que queremos hacer.\n",
    "model = tf.keras.Sequential([\n",
    "                            # Cada capa se lista aquí\n",
    "                            # Definimos las características de la red\n",
    "                            # Dense, nos permite definir la operación matemática a ejecutar xw + b\n",
    "                            # Básicamente aplica la operación: output = activation(dot(input, kernel) + bias\n",
    "                            tf.keras.layers.Dense(output_size,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])\n",
    "\n",
    "# Definimos el ratio de aprendizaje\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Definimos la función de pérdida a minimizar.  En este caso el error cuadrático medio\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_absolute_error')\n",
    "\n",
    "# Definimos el modelo con los datos de entrada y targets.\n",
    "# Ejecutamos 100 ciclos\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f78883",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2644bf",
   "metadata": {},
   "source": [
    "Versión custom con funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargamos el conjunto de datos\n",
    "training_data = np.load('data/data_100_000.npz')\n",
    "\n",
    "# Definimos la función para entrenar el modelo con MAE\n",
    "def train_model_with_mae(training_data, learning_rate):\n",
    "    # Definimos el tamaño de salida del modelo\n",
    "    output_size = 1\n",
    "\n",
    "    # Definimos el modelo\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(output_size,\n",
    "                              kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                              bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "    ])\n",
    "\n",
    "    # Configuramos el optimizador con el learning rate específico\n",
    "    custom_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compilamos el modelo con la función de pérdida MAE\n",
    "    model.compile(optimizer=custom_optimizer, loss='mean_absolute_error')\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    history = model.fit(training_data['inputs'], training_data['targets'], \n",
    "                        epochs=300, verbose=1)  # Cambiar verbose a 0 si no quieres ver los detalles\n",
    "\n",
    "    # Obtenemos el error final (última pérdida)\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    return final_loss\n",
    "\n",
    "# Ejecutamos el entrenamiento con el learning rate 0.01\n",
    "learning_rate = 0.01\n",
    "final_loss = train_model_with_mae(training_data, learning_rate=learning_rate)\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(f\"Pérdida final utilizando MAE con learning rate {learning_rate}: {final_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a0b325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Explicación paso a paso\n",
    "# Cargar datos: El conjunto de datos generado previamente se utiliza directamente.\n",
    "# Modelo: No cambia respecto al ejercicio anterior. Es un modelo secuencial simple con una única capa densa.\n",
    "# Función de pérdida: Se reemplaza mean_squared_error por mean_absolute_error en la función model.compile.\n",
    "# Entrenamiento: Ejecutamos el modelo durante 300 épocas con un learning_rate fijo de 0.01.\n",
    "# Resultado: Al final del entrenamiento, imprimimos la pérdida final (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación con MSE\n",
    "\n",
    "# Función para entrenar con diferentes funciones de pérdida\n",
    "def train_model_with_loss(training_data, learning_rate, loss_function):\n",
    "    output_size = 1\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(output_size,\n",
    "                              kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                              bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "    ])\n",
    "    custom_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=custom_optimizer, loss=loss_function)\n",
    "    history = model.fit(training_data['inputs'], training_data['targets'], epochs=300, verbose=0)\n",
    "    return history.history['loss'][-1]\n",
    "\n",
    "# Comparación de MSE y MAE\n",
    "loss_functions = ['mean_squared_error', 'mean_absolute_error']\n",
    "results = {}\n",
    "\n",
    "for loss_function in loss_functions:\n",
    "    print(f\"Entrenando con función de pérdida: {loss_function}\")\n",
    "    final_loss = train_model_with_loss(training_data, learning_rate=0.01, loss_function=loss_function)\n",
    "    results[loss_function] = final_loss\n",
    "    print(f\"Pérdida final: {final_loss}\")\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"\\nResultados:\")\n",
    "for loss_function, loss in results.items():\n",
    "    print(f\"{loss_function}: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50173ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de la pérdida\n",
    "\n",
    "# Visualización de la pérdida durante las épocas\n",
    "history_mse = train_model_with_loss(training_data, learning_rate=0.01, loss_function='mean_squared_error')\n",
    "history_mae = train_model_with_loss(training_data, learning_rate=0.01, loss_function='mean_absolute_error')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history_mse, label='MSE')\n",
    "plt.plot(history_mae, label='MAE')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Comparación de MSE y MAE durante el entrenamiento')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05236c73",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Crea una nueva función de activación que sea f(x,z) = 13 * xs + 7 * zs -12 y comprueba si el algoritmo funciona de la misma manera.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035881fa",
   "metadata": {},
   "source": [
    "#### **Generamos los datos (1_000)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ebb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = 1_000\n",
    "\n",
    "np.random.seed(123)\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "targets = 13*xs - 7*zs + -12 + noise\n",
    "\n",
    "# Hasta aquí hemos generado los mismos datos, ahora los guardamos\n",
    "np.savez('data/data_1_000', inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28327a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el conjunto de datos\n",
    "training_data = np.load('data/data_1_000.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d88971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos una variable donde guardaremos el total de variables del modelo\n",
    "# En nuestro caso 2 xs y zs\n",
    "#input_size = 2\n",
    "\n",
    "# Declaramos el total de variables de salida del modelo\n",
    "# En nuestro caso 1\n",
    "output_size = 1\n",
    "\n",
    "# Definimos nuestro modelo como secuencial.\n",
    "# En esta etapa no hay datos involucrados, sólo definimos lo que queremos hacer.\n",
    "model = tf.keras.Sequential([\n",
    "                            # Cada capa se lista aquí\n",
    "                            # Definimos las características de la red\n",
    "                            # Dense, nos permite definir la operación matemática a ejecutar xw + b\n",
    "                            # Básicamente aplica la operación: output = activation(dot(input, kernel) + bias\n",
    "                            tf.keras.layers.Dense(output_size,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])\n",
    "\n",
    "# Definimos el ratio de aprendizaje\n",
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Definimos la función de pérdida a minimizar.  En este caso el error cuadrático medio\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Definimos el modelo con los datos de entrada y targets.\n",
    "# Ejecutamos 100 ciclos\n",
    "model.fit(training_data['inputs'], training_data['targets'], epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143681c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bedd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La extracción de pesos y biases es muy sencilla\n",
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6def16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "bias = model.layers[0].get_weights()[1]\n",
    "print(weights)\n",
    "print(bias)\n",
    "\n",
    "# éste es un modelo simple, pero en otros más complicados podemos tener cientos o miles de valores en estas variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c698fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18605f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos predichos\n",
    "model.predict_on_batch(training_data['inputs']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0134184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos reales\n",
    "training_data['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00beeb5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f212cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos np.squeeze para poder ajustar los datos a lo que espera plt.plot\n",
    "plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()\n",
    "\n",
    "# La idea de haber usado TensorFlow para el mismo ejercicio y poder comparar las líneas de código necesarias\n",
    "# para obtener el mismo resultado.  Muchas menos en TensorFlow\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
