cat("R² promedio en datos de entrenamiento:", mean(aciertoRFtrain), "\n")
# Métricas de error
cat("Error medio absoluto (MAE):", mean(abs(ver$Error)), "\n")
cat("Percentiles del error absoluto:", quantile(abs(ver$Error), probs = c(0.05, 0.95)), "\n")
rm(list=ls())
datos = read.csv("hormigon.csv")
aciertoXGBoost = c()
indice = createMultiFolds(datos$strength, k = 5, times = 1)
for (i in 1:length(indice)){
datostrain = datos[ indice[[i]],]
datostst = datos[-indice[[i]],]
train_y=datostrain[,9]
train_x=datostrain[,-9]
test_x=datostst[,-9]
test_y=datostst[,9]
dtrain <- xgb.DMatrix(as.matrix(train_x),label = train_y)
dtest <- xgb.DMatrix(as.matrix(test_x))
xgb_params <- list(colsample_bytree = 1, #how many variables to consider for each tree
subsample = 1, #how much of the data to use for each tree
booster = "gbtree",
max_depth = 9, #how many levels in the tree
min_child_weight=1.5,
reg_alpha=0.8,
reg_lambda=0.6,
seed=42,
eta = 0.03, #shrinkage rate to control overfitting through conservative approach
eval_metric = "rmse",
objective = "reg:squarederror",
gamma = 0)
gb_dt <- xgb.train(xgb_params,dtrain,nfold = 12,nrounds = 1000)
prediccionxgb <- predict(gb_dt,dtest)
resultado=1-(sum((datostst[,9]-prediccionxgb)^2)/sum((datostst[,9]-mean(datostst[,9]))^2))
aciertoXGBoost = rbind(aciertoXGBoost,c(resultado))
}
# Acierto
aciertoXGBoost
mean(aciertoXGBoost)
# Importancia de las variables
imp_matrix <- as.tibble(xgb.importance(feature_names = colnames(test_x), model = gb_dt))
# Nodos terminales
length(unique(prediccionxgb))
length(unique(datostst$strength))
# Error
ver = cbind.data.frame(datostst$strength,prediccionxgb)
ver$Error = ver$`datostst$strength`-ver$prediccionxgb
mean(abs(ver$Error))
quantile(ver$Error,probs = c(0.05,0.95))
plot(ver$`datostst$strength`,ver$prediccion)
'ce' %in% colnames(datos)
library(stringr)
str_detect('cement',colnames(datos))
v =grepl('ce',colnames(datos))
colname(datos)colnames(datos)[v]
which(v)
# Limpiar el entorno
rm(list = ls())
# Cargar paquetes necesarios
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
library(xgboost)
library(caret)
library(dplyr)
# Cargar el dataset Boston
data("Boston", package = "MASS")
datos <- Boston
# Almacenar resultados para el modelo
aciertoXGBoost <- c()
# Crear folds para validación cruzada
indice <- createMultiFolds(datos$medv, k = 5, times = 1)
# Bucle para entrenamiento y evaluación con validación cruzada
for (i in 1:length(indice)) {
# Dividir datos en entrenamiento y prueba
datostrain <- datos[indice[[i]], ]
datostst <- datos[-indice[[i]], ]
# Crear matrices de entrenamiento y prueba para XGBoost
train_y <- datostrain$medv
train_x <- datostrain %>% select(-medv)
test_x <- datostst %>% select(-medv)
test_y <- datostst$medv
dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest <- xgb.DMatrix(data = as.matrix(test_x))
# Parámetros de XGBoost
xgb_params <- list(
colsample_bytree = 1,     # Variables por árbol
subsample = 1,            # Porcentaje de datos para cada árbol
booster = "gbtree",
max_depth = 9,            # Profundidad máxima del árbol
min_child_weight = 1.5,
reg_alpha = 0.8,
reg_lambda = 0.6,
seed = 42,
eta = 0.03,               # Tasa de aprendizaje
eval_metric = "rmse",     # Métrica de evaluación
objective = "reg:squarederror",
gamma = 0
)
# Entrenar el modelo
gb_dt <- xgb.train(
params = xgb_params,
data = dtrain,
nrounds = 1000,
verbose = 0  # Para evitar exceso de mensajes en consola
)
# Predicción en datos de prueba
prediccionxgb <- predict(gb_dt, dtest)
# Calcular R² manualmente
resultado <- 1 - (sum((test_y - prediccionxgb)^2) / sum((test_y - mean(test_y))^2))
# Guardar resultado
aciertoXGBoost <- rbind(aciertoXGBoost, c(resultado))
}
# Resultados finales
cat("R² promedio en validación cruzada:", mean(aciertoXGBoost), "\n")
# Importancia de las variables
imp_matrix <- as_tibble(xgb.importance(feature_names = colnames(train_x), model = gb_dt))
print("Importancia de Variables:")
print(imp_matrix)
# Evaluación de errores en el último fold
ver <- data.frame(
Realidad = test_y,
Predicción = prediccionxgb
)
ver$Error <- ver$Realidad - ver$Predicción
# Métricas de error
cat("Error medio absoluto (MAE):", mean(abs(ver$Error)), "\n")
cat("Percentiles del error absoluto:", quantile(abs(ver$Error), probs = c(0.05, 0.95)), "\n")
# Gráfico de Predicción vs Realidad
plot(
ver$Realidad, ver$Predicción,
xlab = "Valores Reales (medv)", ylab = "Predicciones",
main = "Predicciones vs Realidad"
)
abline(0, 1, col = "red", lwd = 2)  # Línea ideal
# Limpiar el entorno
rm(list = ls())
# Cargar paquetes necesarios
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
library(xgboost)
library(caret)
library(dplyr)
# Cargar el dataset Boston
data("Boston", package = "MASS")
datos <- Boston
# Almacenar resultados para el modelo
aciertoXGBoost <- c()
# Crear folds para validación cruzada
indice <- createMultiFolds(datos$medv, k = 5, times = 1)
# Bucle para entrenamiento y evaluación con validación cruzada
for (i in 1:length(indice)) {
# Dividir datos en entrenamiento y prueba
datostrain <- datos[indice[[i]], ]
datostst <- datos[-indice[[i]], ]
# Crear matrices de entrenamiento y prueba para XGBoost
train_y <- datostrain$medv
train_x <- datostrain %>% select(-medv)
test_x <- datostst %>% select(-medv)
test_y <- datostst$medv
dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest <- xgb.DMatrix(data = as.matrix(test_x))
# Parámetros de XGBoost
xgb_params <- list(
colsample_bytree = 1,     # Variables por árbol
subsample = 1,            # Porcentaje de datos para cada árbol
booster = "gbtree",
max_depth = 9,            # Profundidad máxima del árbol
min_child_weight = 1.5,
reg_alpha = 0.8,
reg_lambda = 0.6,
seed = 42,
eta = 0.03,               # Tasa de aprendizaje
eval_metric = "rmse",     # Métrica de evaluación
objective = "reg:squarederror",
gamma = 0
)
# Entrenar el modelo
gb_dt <- xgb.train(
params = xgb_params,
data = dtrain,
nrounds = 1000,
verbose = 0  # Para evitar exceso de mensajes en consola
)
# Predicción en datos de prueba
prediccionxgb <- predict(gb_dt, dtest)
# Calcular R² manualmente
resultado <- 1 - (sum((test_y - prediccionxgb)^2) / sum((test_y - mean(test_y))^2))
# Guardar resultado
aciertoXGBoost <- rbind(aciertoXGBoost, c(resultado))
}
# Resultados finales
cat("R² promedio en validación cruzada:", mean(aciertoXGBoost), "\n")
# **Selección de Variables (Importancia)**
imp_matrix <- xgb.importance(feature_names = colnames(train_x), model = gb_dt)
print("Importancia de Variables:")
print(imp_matrix)
# Seleccionar las mejores variables (e.g., top 5)
top_variables <- imp_matrix %>% top_n(5, wt = Gain) %>% pull(Feature)
print("Top 5 variables más importantes:")
print(top_variables)
# Crear nuevos datasets solo con las mejores variables
train_x_top <- train_x[, top_variables]
test_x_top <- test_x[, top_variables]
# Crear matrices para XGBoost con las mejores variables
dtrain_top <- xgb.DMatrix(as.matrix(train_x_top), label = train_y)
dtest_top <- xgb.DMatrix(as.matrix(test_x_top))
# Entrenar nuevamente con las mejores variables
gb_dt_top <- xgb.train(
params = xgb_params,
data = dtrain_top,
nrounds = 1000,
verbose = 0
)
# Predicción y evaluación con las mejores variables
prediccionxgb_top <- predict(gb_dt_top, dtest_top)
resultado_top <- 1 - (sum((test_y - prediccionxgb_top)^2) / sum((test_y - mean(test_y))^2))
cat("R² en prueba con las mejores variables:", resultado_top, "\n")
# Evaluación de errores en el último fold con las mejores variables
ver <- data.frame(
Realidad = test_y,
Predicción = prediccionxgb_top
)
ver$Error <- ver$Realidad - ver$Predicción
# Métricas de error
cat("Error medio absoluto (MAE):", mean(abs(ver$Error)), "\n")
cat("Percentiles del error absoluto:", quantile(abs(ver$Error), probs = c(0.05, 0.95)), "\n")
# Gráfico de Predicción vs Realidad
plot(
ver$Realidad, ver$Predicción,
xlab = "Valores Reales (medv)", ylab = "Predicciones",
main = "Predicciones vs Realidad (Mejores Variables)"
)
abline(0, 1, col = "red", lwd = 2)  # Línea ideal
# Limpiar el entorno
rm(list = ls())
# Cargar paquetes necesarios
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
library(xgboost)
library(caret)
library(dplyr)
# Cargar el dataset Boston
data("Boston", package = "MASS")
datos <- Boston
# Almacenar resultados para el modelo
aciertoXGBoost <- c()
# Crear folds para validación cruzada
indice <- createMultiFolds(datos$medv, k = 5, times = 1)
# Bucle para entrenamiento y evaluación con validación cruzada
for (i in 1:length(indice)) {
# Dividir datos en entrenamiento y prueba
datostrain <- datos[indice[[i]], ]
datostst <- datos[-indice[[i]], ]
# Crear matrices de entrenamiento y prueba para XGBoost
train_y <- datostrain$medv
train_x <- datostrain %>% select(-medv)
test_x <- datostst %>% select(-medv)
test_y <- datostst$medv
dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest <- xgb.DMatrix(data = as.matrix(test_x))
# Parámetros de XGBoost
xgb_params <- list(
colsample_bytree = 1,     # Variables por árbol
subsample = 1,            # Porcentaje de datos para cada árbol
booster = "gbtree",
max_depth = 9,            # Profundidad máxima del árbol
min_child_weight = 1.5,
reg_alpha = 0.8,
reg_lambda = 0.6,
seed = 42,
eta = 0.03,               # Tasa de aprendizaje
eval_metric = "rmse",     # Métrica de evaluación
objective = "reg:squarederror",
gamma = 0
)
# Entrenar el modelo
gb_dt <- xgb.train(
params = xgb_params,
data = dtrain,
nrounds = 1000,
verbose = 0  # Para evitar exceso de mensajes en consola
)
# Predicción en datos de prueba
prediccionxgb <- predict(gb_dt, dtest)
# Calcular R² manualmente
resultado <- 1 - (sum((test_y - prediccionxgb)^2) / sum((test_y - mean(test_y))^2))
# Guardar resultado
aciertoXGBoost <- rbind(aciertoXGBoost, c(resultado))
}
# Resultados finales
cat("R² promedio en validación cruzada:", mean(aciertoXGBoost), "\n")
# **Selección de Variables (Importancia)**
imp_matrix <- xgb.importance(feature_names = colnames(train_x), model = gb_dt)
print("Importancia de Variables:")
print(imp_matrix)
# Seleccionar las mejores variables (e.g., top 5)
top_variables <- imp_matrix %>% top_n(5, wt = Gain) %>% pull(Feature)
print("Top 5 variables más importantes:")
print(top_variables)
# Crear nuevos datasets solo con las mejores variables
train_x_top <- train_x[, top_variables]
test_x_top <- test_x[, top_variables]
# Crear matrices para XGBoost con las mejores variables
dtrain_top <- xgb.DMatrix(as.matrix(train_x_top), label = train_y)
dtest_top <- xgb.DMatrix(as.matrix(test_x_top))
# Entrenar nuevamente con las mejores variables
gb_dt_top <- xgb.train(
params = xgb_params,
data = dtrain_top,
nrounds = 1000,
verbose = 0
)
# Predicción y evaluación con las mejores variables
prediccionxgb_top <- predict(gb_dt_top, dtest_top)
resultado_top <- 1 - (sum((test_y - prediccionxgb_top)^2) / sum((test_y - mean(test_y))^2))
cat("R² en prueba con las mejores variables:", resultado_top, "\n")
# Evaluación de errores en el último fold con las mejores variables
ver <- data.frame(
Realidad = test_y,
Predicción = prediccionxgb_top
)
ver$Error <- ver$Realidad - ver$Predicción
# Métricas de error
cat("Error medio absoluto (MAE):", mean(abs(ver$Error)), "\n")
cat("Percentiles del error absoluto:", quantile(abs(ver$Error), probs = c(0.05, 0.95)), "\n")
# Gráfico de Predicción vs Realidad
plot(
ver$Realidad, ver$Predicción,
xlab = "Valores Reales (medv)", ylab = "Predicciones",
main = "Predicciones vs Realidad (Mejores Variables)"
)
abline(0, 1, col = "red", lwd = 2)  # Línea ideal
# Limpiar el entorno
rm(list = ls())
# Cargar paquetes necesarios
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
library(xgboost)
library(caret)
library(dplyr)
# Cargar el dataset Boston
data("Boston", package = "MASS")
datos <- Boston
# Almacenar resultados para el modelo
aciertoXGBoost <- c()
# Crear folds para validación cruzada
indice <- createMultiFolds(datos$medv, k = 5, times = 1)
# Bucle para entrenamiento y evaluación con validación cruzada
for (i in 1:length(indice)) {
# Dividir datos en entrenamiento y prueba
datostrain <- datos[indice[[i]], ]
datostst <- datos[-indice[[i]], ]
# Crear matrices de entrenamiento y prueba para XGBoost
train_y <- datostrain$medv
train_x <- datostrain %>% select(-medv)
test_x <- datostst %>% select(-medv)
test_y <- datostst$medv
dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest <- xgb.DMatrix(data = as.matrix(test_x))
# Parámetros de XGBoost
xgb_params <- list(
colsample_bytree = 1,     # Variables por árbol
subsample = 1,            # Porcentaje de datos para cada árbol
booster = "gbtree",
max_depth = 9,            # Profundidad máxima del árbol
min_child_weight = 1.5,
reg_alpha = 0.8,
reg_lambda = 0.6,
seed = 42,
eta = 0.03,               # Tasa de aprendizaje
eval_metric = "rmse",     # Métrica de evaluación
objective = "reg:squarederror",
gamma = 0
)
# Entrenar el modelo
gb_dt <- xgb.train(
params = xgb_params,
data = dtrain,
nrounds = 1000,
verbose = 0  # Para evitar exceso de mensajes en consola
)
# Predicción en datos de prueba
prediccionxgb <- predict(gb_dt, dtest)
# Calcular R² manualmente
resultado <- 1 - (sum((test_y - prediccionxgb)^2) / sum((test_y - mean(test_y))^2))
# Guardar resultado
aciertoXGBoost <- rbind(aciertoXGBoost, c(resultado))
}
# Resultados finales
cat("R² promedio en validación cruzada:", mean(aciertoXGBoost), "\n")
# Importancia de las variables
imp_matrix <- as_tibble(xgb.importance(feature_names = colnames(train_x), model = gb_dt))
print("Importancia de Variables:")
print(imp_matrix)
# Evaluación de errores en el último fold
ver <- data.frame(
Realidad = test_y,
Predicción = prediccionxgb
)
ver$Error <- ver$Realidad - ver$Predicción
# Métricas de error
cat("Error medio absoluto (MAE):", mean(abs(ver$Error)), "\n")
cat("Percentiles del error absoluto:", quantile(abs(ver$Error), probs = c(0.05, 0.95)), "\n")
# Gráfico de Predicción vs Realidad
plot(
ver$Realidad, ver$Predicción,
xlab = "Valores Reales (medv)", ylab = "Predicciones",
main = "Predicciones vs Realidad"
)
abline(0, 1, col = "red", lwd = 2)  # Línea ideal
# Limpiar el entorno
rm(list = ls())
# Cargar paquetes necesarios
if (!require("xgboost")) install.packages("xgboost")
if (!require("caret")) install.packages("caret")
if (!require("dplyr")) install.packages("dplyr")
library(xgboost)
library(caret)
library(dplyr)
# Cargar el dataset Boston
data("Boston", package = "MASS")
datos <- Boston
# Almacenar resultados para el modelo
aciertoXGBoost <- c()
# Crear folds para validación cruzada
indice <- createMultiFolds(datos$medv, k = 5, times = 1)
# Bucle para entrenamiento y evaluación con validación cruzada
for (i in 1:length(indice)) {
# Dividir datos en entrenamiento y prueba
datostrain <- datos[indice[[i]], ]
datostst <- datos[-indice[[i]], ]
# Crear matrices de entrenamiento y prueba para XGBoost
train_y <- datostrain$medv
train_x <- datostrain %>% select(-medv)
test_x <- datostst %>% select(-medv)
test_y <- datostst$medv
dtrain <- xgb.DMatrix(data = as.matrix(train_x), label = train_y)
dtest <- xgb.DMatrix(data = as.matrix(test_x))
# Parámetros de XGBoost
xgb_params <- list(
colsample_bytree = 1,     # Variables por árbol
subsample = 1,            # Porcentaje de datos para cada árbol
booster = "gbtree",
max_depth = 9,            # Profundidad máxima del árbol
min_child_weight = 1.5,
reg_alpha = 0.8,
reg_lambda = 0.6,
seed = 42,
eta = 0.03,               # Tasa de aprendizaje
eval_metric = "rmse",     # Métrica de evaluación
objective = "reg:squarederror",
gamma = 0
)
# Entrenar el modelo
gb_dt <- xgb.train(
params = xgb_params,
data = dtrain,
nrounds = 1000,
verbose = 0  # Para evitar exceso de mensajes en consola
)
# Predicción en datos de prueba
prediccionxgb <- predict(gb_dt, dtest)
# Calcular R² manualmente
resultado <- 1 - (sum((test_y - prediccionxgb)^2) / sum((test_y - mean(test_y))^2))
# Guardar resultado
aciertoXGBoost <- rbind(aciertoXGBoost, c(resultado))
}
# Resultados finales
cat("R² promedio en validación cruzada:", mean(aciertoXGBoost), "\n")
# **Selección de Variables (Importancia)**
imp_matrix <- xgb.importance(feature_names = colnames(train_x), model = gb_dt)
print("Importancia de Variables:")
print(imp_matrix)
# Seleccionar las mejores variables (e.g., top 5)
top_variables <- imp_matrix %>% top_n(5, wt = Gain) %>% pull(Feature)
print("Top 5 variables más importantes:")
print(top_variables)
# Crear nuevos datasets solo con las mejores variables
train_x_top <- train_x[, top_variables]
test_x_top <- test_x[, top_variables]
# Crear matrices para XGBoost con las mejores variables
dtrain_top <- xgb.DMatrix(as.matrix(train_x_top), label = train_y)
dtest_top <- xgb.DMatrix(as.matrix(test_x_top))
# Entrenar nuevamente con las mejores variables
gb_dt_top <- xgb.train(
params = xgb_params,
data = dtrain_top,
nrounds = 1000,
verbose = 0
)
# Predicción y evaluación con las mejores variables
prediccionxgb_top <- predict(gb_dt_top, dtest_top)
resultado_top <- 1 - (sum((test_y - prediccionxgb_top)^2) / sum((test_y - mean(test_y))^2))
cat("R² en prueba con las mejores variables:", resultado_top, "\n")
# Evaluación de errores en el último fold con las mejores variables
ver <- data.frame(
Realidad = test_y,
Predicción = prediccionxgb_top
)
ver$Error <- ver$Realidad - ver$Predicción
# Métricas de error
cat("Error medio absoluto (MAE):", mean(abs(ver$Error)), "\n")
cat("Percentiles del error absoluto:", quantile(abs(ver$Error), probs = c(0.05, 0.95)), "\n")
# Gráfico de Predicción vs Realidad
plot(
ver$Realidad, ver$Predicción,
xlab = "Valores Reales (medv)", ylab = "Predicciones",
main = "Predicciones vs Realidad (Mejores Variables)"
)
abline(0, 1, col = "red", lwd = 2)  # Línea ideal
